{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceType":"datasetVersion","sourceId":11904337,"datasetId":7483280,"databundleVersionId":12410031},{"sourceType":"datasetVersion","sourceId":1771962,"datasetId":1053211,"databundleVersionId":1809293},{"sourceType":"datasetVersion","sourceId":11918208,"datasetId":7492523,"databundleVersionId":12425503},{"sourceType":"datasetVersion","sourceId":12062428,"datasetId":7583530,"databundleVersionId":12587153}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import","metadata":{"id":"L3oV47WeCKcn"}},{"cell_type":"code","source":"import os\nimport random\n\nimport pandas as pd\nimport numpy as np\n\nfrom PIL import Image\nfrom tqdm import tqdm\n\nfrom sklearn.model_selection import train_test_split\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader, Subset\nimport torchvision.models as models\nimport torchvision.transforms as transforms\nimport torch.nn.functional as F\nfrom torch import nn, optim\n\nfrom sklearn.metrics import log_loss\nimport wandb\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11180,"status":"ok","timestamp":1747893157421,"user":{"displayName":"박진영","userId":"15299328254354703813"},"user_tz":-540},"id":"5VRIs0HCCKco","outputId":"81c5c51f-7a68-430b-a8df-7ec278f2fa4b","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!ls","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install inplace-abn","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!git clone https://github.com/Alibaba-MIIL/TResNet\n%cd TResNet","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Hyperparameter Setting","metadata":{"id":"6hXgRhYKCKcp"}},{"cell_type":"code","source":"CFG = {\n    'IMG_SIZE': 368,\n    'BATCH_SIZE': 32,\n    'EPOCHS': 10,\n    'LEARNING_RATE': 1e-4,\n    'SEED' : 42\n}","metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1747893157442,"user":{"displayName":"박진영","userId":"15299328254354703813"},"user_tz":-540},"id":"xvRCnuRqCKcp","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!wandb login 1128bf290760f1cd846378e87ab026431fee90c3","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize wandb\nwandb.init(\n    entity='Dacon_Car',\n    project=\"car-classification\",  # your project name\n    name='TResNet_cutout',\n    config=CFG  # this will log your hyperparameters\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Fixed RandomSeed","metadata":{"id":"ullOr-KjCKcp"}},{"cell_type":"code","source":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)    \n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nseed_everything(CFG['SEED']) # Seed 고정","metadata":{"executionInfo":{"elapsed":32,"status":"ok","timestamp":1747893157484,"user":{"displayName":"박진영","userId":"15299328254354703813"},"user_tz":-540},"id":"Uwop2i4qCKcp","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CustomDataset","metadata":{"id":"AUwzuA7jCKcq"}},{"cell_type":"code","source":"import os\nfrom PIL import Image\nimport numpy as np # NumPy 임포트 추가\nfrom torch.utils.data import Dataset\n# albumentations와 ToTensorV2 임포트는 Dataset 클래스 외부에서 이루어져야 합니다.\n# import albumentations as A\n# from albumentations.pytorch import ToTensorV2\n\nclass CustomImageDataset(Dataset):\n    def __init__(self, root_dir, transform=None, is_test=False):\n        self.root_dir = root_dir\n        self.transform = transform\n        self.is_test = is_test\n        self.samples = []\n\n        if is_test:\n            # 테스트셋: 라벨 없이 이미지 경로만 저장\n            for fname in sorted(os.listdir(root_dir)):\n                if fname.lower().endswith(('.jpg', '.jpeg', '.png', '.gif')): # 이미지 확장자 추가\n                    img_path = os.path.join(root_dir, fname)\n                    self.samples.append((img_path,))\n        else:\n            # 학습셋: 클래스별 폴더 구조에서 라벨 추출\n            self.classes = sorted(os.listdir(root_dir))\n            self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}\n\n            for cls_name in self.classes:\n                cls_folder = os.path.join(root_dir, cls_name)\n                # 폴더가 아닌 파일이 있을 수 있으므로 isdir 체크 추가\n                if not os.path.isdir(cls_folder):\n                    continue\n                for fname in os.listdir(cls_folder):\n                    if fname.lower().endswith(('.jpg', '.jpeg', '.png', '.gif')): # 이미지 확장자 추가\n                        img_path = os.path.join(cls_folder, fname)\n                        label = self.class_to_idx[cls_name]\n                        self.samples.append((img_path, label))\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        if self.is_test:\n            img_path = self.samples[idx][0]\n            image = Image.open(img_path).convert('RGB')\n            # PIL 이미지를 NumPy 배열로 변환\n            image = np.array(image)\n\n            if self.transform:\n                # Albumentations는 딕셔너리를 반환하며 'image' 키에 변환된 이미지가 있습니다.\n                transformed_data = self.transform(image=image)\n                image = transformed_data['image'] # PyTorch 텐서 (C, H, W)\n\n            return image\n        else:\n            img_path, label = self.samples[idx]\n            image = Image.open(img_path).convert('RGB')\n            # PIL 이미지를 NumPy 배열로 변환\n            image = np.array(image)\n\n            if self.transform:\n                # Albumentations는 딕셔너리를 반환하며 'image' 키에 변환된 이미지가 있습니다.\n                transformed_data = self.transform(image=image)\n                image = transformed_data['image'] # PyTorch 텐서 (C, H, W)\n\n            return image, label","metadata":{"executionInfo":{"elapsed":31,"status":"ok","timestamp":1747893157488,"user":{"displayName":"박진영","userId":"15299328254354703813"},"user_tz":-540},"id":"EOA2BdsbCKcq","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data Load","metadata":{"id":"6bmib4EdCKcq"}},{"cell_type":"code","source":"train_root = '/kaggle/input/vehicle-upscaled/upscaled/upscaled/train'\ntest_root = '/kaggle/input/vehicle-upscaled/upscaled_test/upscaled/test'","metadata":{"executionInfo":{"elapsed":28,"status":"ok","timestamp":1747893157492,"user":{"displayName":"박진영","userId":"15299328254354703813"},"user_tz":-540},"id":"GLNBNSyDCKcq","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install --upgrade albumentations","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import albumentations as A\nfrom albumentations.pytorch import ToTensorV2 # PyTorch 텐서로 변환하기 위함\nimport numpy as np # Albumentations는 NumPy 배열을 입력으로 받습니다.\nfrom PIL import Image # 이미지 로딩을 위한 라이브러리\n\n# Albumentations의 train_transform\ntrain_transform = A.Compose([\n    # ResizeIfPadNeeded는 가로세로 비율을 유지하면서 이미지의 긴 변 또는 짧은 변을 리사이즈한 다음,\n    # 지정된 크기에 맞춰 패딩을 추가합니다.\n    # pad_height, pad_width는 최종 출력 크기를 의미합니다.\n    # 만약 원본 비율을 유지하면서 패딩으로 채우는 것이 목적이라면 아래와 같이 LongestMaxSize와 PadIfNeeded를 사용합니다.\n    A.LongestMaxSize(max_size=CFG['IMG_SIZE'], interpolation=Image.BILINEAR),\n    A.PadIfNeeded(min_height=CFG['IMG_SIZE'], min_width=CFG['IMG_SIZE'],\n                border_mode=0, fill=(0,0,0)), # border_mode=0 (CONSTANT), value는 패딩 색상\n\n    # 일반적으로 학습 시에는 Resize 후 Normalize를 많이 사용합니다.\n    # torchvision의 Normalize와 동일한 mean/std 값을 사용합니다.\n    A.Normalize(mean=[0.485, 0.456, 0.406],\n                std=[0.229, 0.224, 0.225],\n                max_pixel_value=255.0), # 이미지 픽셀 값의 최댓값 (일반적으로 255)\n\n    # Albumentations의 ToTensorV2는 이미지를 PyTorch 텐서로 변환하고 채널 순서를 (H, W, C) -> (C, H, W)로 변경합니다.\n    # torchvision의 ToTensor()와 유사하게 동작합니다.\n    ToTensorV2()\n])  \n\n# Albumentations의 val_transform (train_transform과 동일하게 구성)\nval_transform = A.Compose([\n    # 검증 시에도 동일하게 Resize 및 Normalize를 적용합니다.\n    A.LongestMaxSize(max_size=CFG['IMG_SIZE'], interpolation=Image.BILINEAR),\n    A.PadIfNeeded(min_height=CFG['IMG_SIZE'], min_width=CFG['IMG_SIZE'],\n                border_mode=0, fill=(0,0,0)), # border_mode=0 (CONSTANT), value는 패딩 색상\n    A.Normalize(mean=[0.485, 0.456, 0.406],\n                std=[0.229, 0.224, 0.225],\n                max_pixel_value=255.0),\n    ToTensorV2()    \n])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 전체 데이터셋 로드\nfull_dataset = CustomImageDataset(train_root, transform=None)\nprint(f\"총 이미지 수: {len(full_dataset)}\")\n\ntargets = [label for _, label in full_dataset.samples]\nclass_names = full_dataset.classes\n\n# Stratified Split\ntrain_idx, val_idx = train_test_split(\n    range(len(targets)), test_size=0.2, stratify=targets, random_state=42\n)\n\n# Subset + transform 각각 적용  \ntrain_dataset = Subset(CustomImageDataset(train_root, transform=train_transform), train_idx)\nval_dataset = Subset(CustomImageDataset(train_root, transform=val_transform), val_idx)\nprint(f'train 이미지 수: {len(train_dataset)}, valid 이미지 수: {len(val_dataset)}')\n\n\n# DataLoader 정의\ntrain_loader = DataLoader(train_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model Define","metadata":{"id":"fM0RBKa5CKcr"}},{"cell_type":"code","source":"from src.models.tresnet_v2.tresnet_v2 import TResnetL_V2 as TResnetL368\n\n\nclass TResNet(nn.Module):\n    def __init__(self, num_classes):\n        super(TResNet, self).__init__()\n        model_params = {'num_classes' : 196}\n        self.backbone = TResnetL368(model_params)\n        \n        weights_path = \"/kaggle/input/tresnet-stanford-cars-pretrained/stanford_cars_tresnet-l-v2_96_27.pth\"\n        pretrained_weights = torch.load(weights_path)\n        \n        self.backbone.load_state_dict(pretrained_weights['model'])  # TResnetL368 모델 불러오기\n        self.feature_dim = self.backbone.num_features\n        self.backbone.head = nn.Identity()  # feature extractor로만 사용\n        self.head = nn.Linear(self.feature_dim, num_classes)  # 분류기\n\n    def forward(self, x):\n        x = self.backbone(x)\n        x = self.head(x)\n        return x","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.nn.modules.batchnorm import _BatchNorm\n\ndef cosine_anneal_schedule(t, nb_epoch, lr):\n    cos_inner = np.pi * (t % (nb_epoch))\n    cos_inner /= (nb_epoch)\n    cos_out = np.cos(cos_inner) + 1\n\n    return float(lr / 2 * cos_out)\n\nclass BasicConv(nn.Module):\n    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0, dilation=1, groups=1, relu=True, bn=True, bias=False):\n        super(BasicConv, self).__init__()\n        self.out_channels = out_planes\n        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size,\n                              stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n        self.bn = nn.BatchNorm2d(out_planes, eps=1e-5,\n                                 momentum=0.01, affine=True) if bn else None\n        self.relu = nn.ReLU() if relu else None\n\n    def forward(self, x):\n        x = self.conv(x)\n        if self.bn is not None:\n            x = self.bn(x)\n        if self.relu is not None:\n            x = self.relu(x)\n        return x\n\nclass Features(nn.Module):\n    def __init__(self, net_layers_FeatureHead):\n        super(Features, self).__init__()\n        self.net_layer_0 = nn.Sequential(net_layers_FeatureHead[0])\n        self.net_layer_1 = nn.Sequential(*net_layers_FeatureHead[1])\n        self.net_layer_2 = nn.Sequential(*net_layers_FeatureHead[2])\n        self.net_layer_3 = nn.Sequential(*net_layers_FeatureHead[3])\n        self.net_layer_4 = nn.Sequential(*net_layers_FeatureHead[4])\n        self.net_layer_5 = nn.Sequential(*net_layers_FeatureHead[5])\n\n    def forward(self, x):\n        x = self.net_layer_0(x)\n        x = self.net_layer_1(x)\n        x = self.net_layer_2(x)\n        x1 = self.net_layer_3(x)\n        x2 = self.net_layer_4(x1)\n        x3 = self.net_layer_5(x2)\n\n        return x1, x2, x3\n\n\nclass Network_Wrapper(nn.Module):\n    def __init__(self, net_layers, num_classes, classifier):\n        super().__init__()\n        self.Features = Features(net_layers)\n        self.classifier_pool = nn.Sequential(classifier[0])\n        \n        # classifier_initial을 num_classes에 맞게 수정\n        self.classifier_initial = nn.Linear(2048, num_classes)  # 기존 196을 num_classes로 변경\n        \n        self.sigmoid = nn.Sigmoid()\n        self.lrelu = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n\n        self.max_pool1 = nn.MaxPool2d(kernel_size=46, stride=1)\n        self.max_pool2 = nn.MaxPool2d(kernel_size=23, stride=1)\n        self.max_pool3 = nn.MaxPool2d(kernel_size=12, stride=1)\n\n        self.conv_block1 = nn.Sequential(\n            BasicConv(512, 512, kernel_size=1, stride=1, padding=0, relu=True),\n            BasicConv(512, 1024, kernel_size=3, stride=1, padding=1, relu=True)\n        )\n        self.classifier1 = nn.Sequential(\n            nn.BatchNorm1d(1024),\n            nn.Linear(1024, 512),\n            nn.BatchNorm1d(512),\n            nn.ELU(inplace=True),\n            nn.Linear(512, num_classes)\n        )\n\n        self.conv_block2 = nn.Sequential(\n            BasicConv(1024, 512, kernel_size=1, stride=1, padding=0, relu=True),\n            BasicConv(512, 1024, kernel_size=3, stride=1, padding=1, relu=True)\n        )\n        self.classifier2 = nn.Sequential(\n            nn.BatchNorm1d(1024),\n            nn.Linear(1024, 512),\n            nn.BatchNorm1d(512),\n            nn.ELU(inplace=True),\n            nn.Linear(512, num_classes),\n        )\n\n        self.conv_block3 = nn.Sequential(\n            BasicConv(2048, 512, kernel_size=1, stride=1, padding=0, relu=True),\n            BasicConv(512, 1024, kernel_size=3, stride=1, padding=1, relu=True)\n        )\n        self.classifier3 = nn.Sequential(\n            nn.BatchNorm1d(1024),\n            nn.Linear(1024, 512),\n            nn.BatchNorm1d(512),\n            nn.ELU(inplace=True),\n            nn.Linear(512, num_classes),\n        )\n\n    def forward(self, x):\n        _, _, x3 = self.Features(x) # , x2, x3\n        # map1 = x1.clone()\n        # map2 = x2.clone()\n        # map3 = x3.clone()\n\n        classifiers = self.classifier_pool(x3).view(x3.size(0), -1)\n        classifiers = self.classifier_initial(classifiers)  # 이제 num_classes 출력\n\n        # x1_ = self.conv_block1(x1)\n        # x1_ = self.max_pool1(x1_)\n        # x1_f = x1_.view(x1_.size(0), -1)\n\n        # x1_c = self.classifier1(x1_f)\n\n        # x2_ = self.conv_block2(x2)\n        # x2_ = self.max_pool2(x2_)\n        # x2_f = x2_.view(x2_.size(0), -1)\n        # x2_c = self.classifier2(x2_f)\n\n        # x3_ = self.conv_block3(x3)\n        # x3_ = self.max_pool3(x3_)\n        # x3_f = x3_.view(x3_.size(0), -1)\n        # x3_c = self.classifier3(x3_f)\n\n        return classifiers #x1_c , x2_c, x3_c , map1, map2, map3\n\n\nclass Anti_Noise_Decoder(nn.Module):\n    def __init__(self, scale, in_channel):\n        super(Anti_Noise_Decoder, self).__init__()\n        self.Sigmoid = nn.Sigmoid()\n\n        in_channel = in_channel // (scale * scale)\n\n        self.skip = nn.Sequential(\n            nn.Conv2d(3, 64, 3, 1, 1, bias=False),\n            nn.LeakyReLU(negative_slope=0.1, inplace=True),\n            nn.Conv2d(64, 3, 3, 1, 1, bias=False),\n            nn.LeakyReLU(negative_slope=0.1, inplace=True)\n\n        )\n\n        self.process = nn.Sequential(\n            nn.PixelShuffle(scale),\n            nn.Conv2d(in_channel, 256, 3, 1, 1, bias=False),\n            nn.LeakyReLU(negative_slope=0.1, inplace=True),\n            nn.PixelShuffle(2),\n            nn.Conv2d(64, 128, 3, 1, 1, bias=False),\n            nn.LeakyReLU(negative_slope=0.1, inplace=True),\n            nn.PixelShuffle(2),\n            nn.Conv2d(32, 64, 3, 1, 1, bias=False),\n            nn.LeakyReLU(negative_slope=0.1, inplace=True),\n            nn.PixelShuffle(2),\n            nn.Conv2d(16, 3, 3, 1, 1, bias=False),\n            nn.LeakyReLU(negative_slope=0.1, inplace=True)\n        )\n\n    def forward(self, x, map):\n        x_ = self.process(map)\n        if not (x.size() == x_.size()):\n            x_ = F.interpolate(x, (x.size(2),x.size(3)), mode='bilinear')\n        return self.skip(x) + x_\n\n\ndef img_add_noise(x, transformation_seq):\n    x = x.permute(0, 2, 3, 1)\n    x = x.cpu().numpy()\n    x = transformation_seq(images=x)\n    x = torch.from_numpy(x.astype(np.float32))\n    x = x.permute(0, 3, 1, 2)\n    return x\n\ndef smooth_crossentropy(pred, gold, smoothing=0.1):\n    n_class = pred.size(1)\n\n    one_hot = torch.full_like(pred, fill_value=smoothing / (n_class - 1))\n    one_hot.scatter_(dim=1, index=gold.unsqueeze(1), value=1.0 - smoothing)\n    log_prob = F.log_softmax(pred, dim=1)\n\n    return F.kl_div(input=log_prob, target=one_hot, reduction='none').sum(-1)\n\ndef CELoss(x, y):\n    return smooth_crossentropy(x, y, smoothing=0.1)\n\nclass CharbonnierLoss(nn.Module):\n    \"\"\"Charbonnier Loss (L1)\"\"\"\n\n    def __init__(self, eps=1e-3):\n        super(CharbonnierLoss, self).__init__()\n        self.eps = eps\n\n    def forward(self, x, y):\n        diff = x - y\n        # loss = torch.sum(torch.sqrt(diff * diff + self.eps))\n        loss = torch.mean(torch.sqrt((diff * diff) + (self.eps*self.eps)))\n        return loss\n    \n\n\n\ndef disable_running_stats(model):\n    def _disable(module):\n        if isinstance(module, _BatchNorm):\n            module.backup_momentum = module.momentum\n            module.momentum = 0\n\n    model.apply(_disable)\n\ndef enable_running_stats(model):\n    def _enable(module):\n        if isinstance(module, _BatchNorm) and hasattr(module, \"backup_momentum\"):\n            module.momentum = module.backup_momentum\n\n    model.apply(_enable)\n\n\nclass Student_Wrapper(nn.Module):\n    def __init__(self, net_layers, classifier):\n        super(Student_Wrapper, self).__init__()\n        self.net_layer_0 = nn.Sequential(net_layers[0])\n        self.net_layer_1 = nn.Sequential(*net_layers[1])\n        self.net_layer_2 = nn.Sequential(*net_layers[2])\n        self.net_layer_3 = nn.Sequential(*net_layers[3])\n        self.net_layer_4 = nn.Sequential(*net_layers[4])\n        self.net_layer_5 = nn.Sequential(*net_layers[5])\n\n        self.classifier_pool = nn.Sequential(classifier[0])\n        self.classifier_initial = nn.Sequential(classifier[1])\n\n    def forward(self, x):\n        x = self.net_layer_0(x)\n        x = self.net_layer_1(x)\n        x = self.net_layer_2(x)\n        x1 = self.net_layer_3(x)\n        x2 = self.net_layer_4(x1)\n        x3 = self.net_layer_5(x2)\n\n\n        classifiers = self.classifier_pool(x3).view(x3.size(0), -1)\n        out = self.classifier_initial(classifiers)\n\n        return out, x1, x2, x3","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# SAM\nclass SAM(torch.optim.Optimizer):\n    def __init__(self, params, base_optimizer, rho=0.05, adaptive=False, **kwargs):\n        assert rho >= 0.0, f\"Invalid rho, should be non-negative: {rho}\"\n\n        defaults = dict(rho=rho, adaptive=adaptive, **kwargs)\n        super(SAM, self).__init__(params, defaults)\n\n        self.base_optimizer = base_optimizer(self.param_groups, **kwargs)\n        self.param_groups = self.base_optimizer.param_groups\n        self.defaults.update(self.base_optimizer.defaults)\n\n    @torch.no_grad()\n    def first_step(self, zero_grad=False):\n        grad_norm = self._grad_norm()\n        for group in self.param_groups:\n            scale = group[\"rho\"] / (grad_norm + 1e-12)\n\n            for p in group[\"params\"]:\n                if p.grad is None: continue\n                self.state[p][\"old_p\"] = p.data.clone()\n                e_w = (torch.pow(p, 2) if group[\"adaptive\"] else 1.0) * p.grad * scale.to(p)\n                p.add_(e_w)  # climb to the local maximum \"w + e(w)\"\n\n        if zero_grad: self.zero_grad()\n\n    @torch.no_grad()\n    def second_step(self, zero_grad=False):\n        for group in self.param_groups:\n            for p in group[\"params\"]:\n                if p.grad is None: continue\n                p.data = self.state[p][\"old_p\"]  # get back to \"w\" from \"w + e(w)\"\n\n        self.base_optimizer.step()  # do the actual \"sharpness-aware\" update\n\n        if zero_grad: self.zero_grad()\n\n    @torch.no_grad()\n    def step(self, closure=None):\n        assert closure is not None, \"Sharpness Aware Minimization requires closure, but it was not provided\"\n        closure = torch.enable_grad()(closure)  # the closure should do a full forward-backward pass\n\n        self.first_step(zero_grad=True)\n        closure()\n        self.second_step()\n\n    def _grad_norm(self):\n        shared_device = self.param_groups[0][\"params\"][0].device  # put everything on the same device, in case of model parallelism\n        norm = torch.norm(\n                    torch.stack([\n                        ((torch.abs(p) if group[\"adaptive\"] else 1.0) * p.grad).norm(p=2).to(shared_device)\n                        for group in self.param_groups for p in group[\"params\"]\n                        if p.grad is not None\n                    ]),\n                    p=2\n               )\n        return norm\n\n    def load_state_dict(self, state_dict):\n        super().load_state_dict(state_dict)\n        self.base_optimizer.param_groups = self.param_groups","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Train/ Validation","metadata":{"id":"NGlK2nPYCKcr"}},{"cell_type":"code","source":"# model = TResNet(num_classes=len(class_names)).to(device)\nbest_logloss = float('inf')\n\n# 손실 함수\ncriterion = nn.CrossEntropyLoss()\n\n# PMAL\nmodel_params = {'num_classes' : 196}\nmodel = TResnetL368(model_params)\nweights_path = \"/kaggle/input/tresnet-stanford-cars-pretrained/stanford_cars_tresnet-l-v2_96_27.pth\"\npretrained_weights = torch.load(weights_path)\nmodel.load_state_dict(pretrained_weights['model'])\n\nnet_layers = list(model.children())\nclassifier = net_layers[1:3]\nnet_layers = net_layers[0]\nnet_layers = list(net_layers.children())\n\n# Network_Wrapper 생성\nmodel = Network_Wrapper(net_layers, len(class_names), classifier).to(device)\n\n# 옵티마이저\noptimizer = optim.Adam(model.parameters(), lr=CFG['LEARNING_RATE'])\n\n# 학습 및 검증 루프\nfor epoch in range(CFG['EPOCHS']):\n    # Train\n    model.train()\n    train_loss = 0.0\n    for images, labels in tqdm(train_loader, desc=f\"[Epoch {epoch+1}/{CFG['EPOCHS']}] Training\"):\n        images, labels = images.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(images)  # logits\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item()\n\n    avg_train_loss = train_loss / len(train_loader)\n\n    # Validation\n    model.eval()\n    val_loss = 0.0\n    correct = 0\n    total = 0\n    all_probs = []\n    all_labels = []\n    all_pred_labels = []\n    class_correct = [0 for _ in range(len(class_names))]\n    class_total = [0 for _ in range(len(class_names))]\n    \n\n    with torch.no_grad():\n        for images, labels in tqdm(val_loader, desc=f\"[Epoch {epoch+1}/{CFG['EPOCHS']}] Validation\"):\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            val_loss += loss.item()\n\n            # Accuracy\n            _, preds = torch.max(outputs, 1)\n            correct += (preds == labels).sum().item()\n            total += labels.size(0)\n\n            for label, pred in zip(labels.cpu().numpy(), preds.cpu().numpy()):\n                class_total[label] += 1\n                if label == pred:\n                    class_correct[label] += 1\n            \n\n            # LogLoss\n            probs = F.softmax(outputs, dim=1)\n            all_probs.extend(probs.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n            all_pred_labels.extend(preds.cpu().numpy())\n\n    avg_val_loss = val_loss / len(val_loader)\n    val_accuracy = 100 * correct / total\n    val_logloss = log_loss(all_labels, all_probs, labels=list(range(len(class_names))))\n\n    class_acc = [c / t if t > 0 else 0 for c, t in zip(class_correct, class_total)]\n    df_class_acc = pd.DataFrame({\n        'Class Name': class_names,\n        'Correct': class_correct,\n        'Total': class_total,\n        'Accuracy': class_acc\n    }).sort_values(by='Accuracy', ascending=False).reset_index(drop=True)\n\n    df_pred_detail = pd.DataFrame({\n        'TrueLabel': all_labels,\n        'PredLabel': all_pred_labels,\n        'TrueClass': [class_names[i] for i in all_labels],\n        'PredClass': [class_names[i] for i in all_pred_labels]\n    })\n    df_pred_detail.to_csv(f'val_pred_detail_epoch_{epoch+1}.csv', index=False, encoding='utf-8-sig')\n    # epoch 번호를 파일명에 포함하여 저장\n    df_class_acc.to_csv(f'class_acc_epoch_{epoch+1}.csv', index=False, encoding='utf-8-sig')\n    # wandb \n    wandb.log({\n        \"train_loss\": avg_train_loss,\n        \"val_loss\": avg_val_loss,\n        \"val_accuracy\": val_accuracy,\n        \"val_logloss\": val_logloss\n    })\n    \n    # 결과 출력\n    print(f\"Train Loss : {avg_train_loss:.4f} || Valid Loss : {avg_val_loss:.4f} | Valid Accuracy : {val_accuracy:.4f}%\")\n\n    # Best model 저장\n    if val_logloss < best_logloss:\n        best_logloss = val_logloss\n        torch.save(model.state_dict(), f'best_model.pth')\n        print(f\"📦 Best model saved at epoch {epoch+1} (logloss: {val_logloss:.4f})\")","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":522},"executionInfo":{"elapsed":263293,"status":"error","timestamp":1747893498648,"user":{"displayName":"박진영","userId":"15299328254354703813"},"user_tz":-540},"id":"DwNWkTC3CKcr","outputId":"c3e4f7fa-70b2-4225-bcc9-b185a00f9f99","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Inference","metadata":{"id":"TwGAOAiKCKcr"}},{"cell_type":"code","source":"test_dataset = CustomImageDataset(test_root, transform=val_transform, is_test=True)\ntest_loader = DataLoader(test_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False)","metadata":{"executionInfo":{"elapsed":379040,"status":"aborted","timestamp":1747893498644,"user":{"displayName":"박진영","userId":"15299328254354703813"},"user_tz":-540},"id":"4vS6URwRCKcr","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 저장된 모델 로드\nmodel_params = {'num_classes' : 196}\nmodel = TResnetL368(model_params)\n\nnet_layers = list(model.children())\nclassifier = net_layers[1:3]\nnet_layers = net_layers[0]\nnet_layers = list(net_layers.children())\n\n# Network_Wrapper 생성\nmodel = Network_Wrapper(net_layers, len(class_names), classifier).to(device)\n\nmodel.load_state_dict(torch.load('best_model.pth', map_location=device))\nmodel.to(device)\n\n# 추론\nmodel.eval()\nresults = []\n\nwith torch.no_grad():\n    for images in test_loader:\n        images = images.to(device)\n        outputs = model(images)\n        probs = F.softmax(outputs, dim=1)\n\n        # 각 배치의 확률을 리스트로 변환\n        for prob in probs.cpu():  # prob: (num_classes,)\n            result = {\n                class_names[i]: prob[i].item()\n                for i in range(len(class_names))\n            }\n            results.append(result)\n\npred = pd.DataFrame(results)","metadata":{"executionInfo":{"elapsed":379037,"status":"aborted","timestamp":1747893498646,"user":{"displayName":"박진영","userId":"15299328254354703813"},"user_tz":-540},"id":"i8XWppr-CKcr","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Submission","metadata":{"id":"HKj-nq9RCKcr"}},{"cell_type":"code","source":"submission = pd.read_csv('/kaggle/input/car-classification/sample_submission.csv', encoding='utf-8-sig')\n\n# 'ID' 컬럼을 제외한 클래스 컬럼 정렬\nclass_columns = submission.columns[1:]\npred = pred[class_columns]\n\nsubmission[class_columns] = pred.values\nsubmission.to_csv('baseline_submission.csv', index=False, encoding='utf-8-sig')","metadata":{"executionInfo":{"elapsed":379034,"status":"aborted","timestamp":1747893498647,"user":{"displayName":"박진영","userId":"15299328254354703813"},"user_tz":-540},"id":"9VcLATLfCKcr","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}