{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L3oV47WeCKcn"
   },
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11180,
     "status": "ok",
     "timestamp": 1747893157421,
     "user": {
      "displayName": "박진영",
      "userId": "15299328254354703813"
     },
     "user_tz": -540
    },
    "id": "5VRIs0HCCKco",
    "outputId": "81c5c51f-7a68-430b-a8df-7ec278f2fa4b",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install inplace-abn\n",
    "!pip install imgaug\n",
    "!pip install early_stopping_pytorch\n",
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import imgaug as ia\n",
    "import imgaug.augmenters as iaa\n",
    "from early_stopping_pytorch import EarlyStopping\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/Alibaba-MIIL/TResNet\n",
    "%cd TResNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6hXgRhYKCKcp"
   },
   "source": [
    "# Hyperparameter Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1747893157442,
     "user": {
      "displayName": "박진영",
      "userId": "15299328254354703813"
     },
     "user_tz": -540
    },
    "id": "xvRCnuRqCKcp",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'IMG_SIZE': 368,\n",
    "    'BATCH_SIZE': 16,\n",
    "    'EPOCHS': 20,\n",
    "    'LEARNING_RATE': 1e-4,\n",
    "    'SEED' : 42,\n",
    "    'gradient_accumulation' : True,\n",
    "    'accumulation_steps' : 4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!wandb login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Initialize wandb\n",
    "wandb.init(\n",
    "    entity='Dacon_Car',\n",
    "    project=\"car-classification\",  # your project name\n",
    "    name='TResNet_PMAL',\n",
    "    config=CFG  # this will log your hyperparameters\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ullOr-KjCKcp"
   },
   "source": [
    "# Fixed RandomSeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 32,
     "status": "ok",
     "timestamp": 1747893157484,
     "user": {
      "displayName": "박진영",
      "userId": "15299328254354703813"
     },
     "user_tz": -540
    },
    "id": "Uwop2i4qCKcp",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)    \n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(CFG['SEED']) # Seed 고정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AUwzuA7jCKcq"
   },
   "source": [
    "# CustomDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 31,
     "status": "ok",
     "timestamp": 1747893157488,
     "user": {
      "displayName": "박진영",
      "userId": "15299328254354703813"
     },
     "user_tz": -540
    },
    "id": "EOA2BdsbCKcq",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np # NumPy 임포트 추가\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, is_test=False):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.is_test = is_test\n",
    "        self.samples = []\n",
    "\n",
    "        if is_test:\n",
    "            # 테스트셋: 라벨 없이 이미지 경로만 저장\n",
    "            for fname in sorted(os.listdir(root_dir)):\n",
    "                if fname.lower().endswith(('.jpg', '.jpeg', '.png', '.gif')): # 이미지 확장자 추가\n",
    "                    img_path = os.path.join(root_dir, fname)\n",
    "                    self.samples.append((img_path,))\n",
    "        else:\n",
    "            # 학습셋: 클래스별 폴더 구조에서 라벨 추출\n",
    "            self.classes = sorted(os.listdir(root_dir))\n",
    "            self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}\n",
    "\n",
    "            for cls_name in self.classes:\n",
    "                cls_folder = os.path.join(root_dir, cls_name)\n",
    "                # 폴더가 아닌 파일이 있을 수 있으므로 isdir 체크 추가\n",
    "                if not os.path.isdir(cls_folder):\n",
    "                    continue\n",
    "                for fname in os.listdir(cls_folder):\n",
    "                    if fname.lower().endswith(('.jpg', '.jpeg', '.png', '.gif')): # 이미지 확장자 추가\n",
    "                        img_path = os.path.join(cls_folder, fname)\n",
    "                        label = self.class_to_idx[cls_name]\n",
    "                        self.samples.append((img_path, label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.is_test:\n",
    "            img_path = self.samples[idx][0]\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            # PIL 이미지를 NumPy 배열로 변환\n",
    "            image = np.array(image)\n",
    "\n",
    "            if self.transform:\n",
    "                # Albumentations는 딕셔너리를 반환하며 'image' 키에 변환된 이미지가 있습니다.\n",
    "                transformed_data = self.transform(image=image)\n",
    "                image = transformed_data['image'] # PyTorch 텐서 (C, H, W)\n",
    "\n",
    "            return image\n",
    "        else:\n",
    "            img_path, label = self.samples[idx]\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            # PIL 이미지를 NumPy 배열로 변환\n",
    "            image = np.array(image)\n",
    "\n",
    "            if self.transform:\n",
    "                # Albumentations는 딕셔너리를 반환하며 'image' 키에 변환된 이미지가 있습니다.\n",
    "                transformed_data = self.transform(image=image)\n",
    "                image = transformed_data['image'] # PyTorch 텐서 (C, H, W)\n",
    "\n",
    "            return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6bmib4EdCKcq"
   },
   "source": [
    "# Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1747893157492,
     "user": {
      "displayName": "박진영",
      "userId": "15299328254354703813"
     },
     "user_tz": -540
    },
    "id": "GLNBNSyDCKcq",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_root = '/kaggle/input/car-classification/train'\n",
    "test_root = '/kaggle/input/car-classification/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2 # PyTorch 텐서로 변환하기 위함\n",
    "import numpy as np # Albumentations는 NumPy 배열을 입력으로 받습니다.\n",
    "from PIL import Image # 이미지 로딩을 위한 라이브러리\n",
    "\n",
    "# Albumentations의 train_transform\n",
    "train_transform = A.Compose([\n",
    "    # ResizeIfPadNeeded는 가로세로 비율을 유지하면서 이미지의 긴 변 또는 짧은 변을 리사이즈한 다음,\n",
    "    # 지정된 크기에 맞춰 패딩을 추가합니다.\n",
    "    # pad_height, pad_width는 최종 출력 크기를 의미합니다.\n",
    "    # 만약 원본 비율을 유지하면서 패딩으로 채우는 것이 목적이라면 아래와 같이 LongestMaxSize와 PadIfNeeded를 사용합니다.\n",
    "    A.LongestMaxSize(max_size=CFG['IMG_SIZE'], interpolation=Image.BILINEAR),\n",
    "    A.PadIfNeeded(min_height=CFG['IMG_SIZE'], min_width=CFG['IMG_SIZE'],\n",
    "                border_mode=0, fill=(0,0,0)), # border_mode=0 (CONSTANT), value는 패딩 색상\n",
    "\n",
    "    # 일반적으로 학습 시에는 Resize 후 Normalize를 많이 사용합니다.\n",
    "    # torchvision의 Normalize와 동일한 mean/std 값을 사용합니다.\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225],\n",
    "                max_pixel_value=255.0), # 이미지 픽셀 값의 최댓값 (일반적으로 255)\n",
    "\n",
    "    # Albumentations의 ToTensorV2는 이미지를 PyTorch 텐서로 변환하고 채널 순서를 (H, W, C) -> (C, H, W)로 변경합니다.\n",
    "    # torchvision의 ToTensor()와 유사하게 동작합니다.\n",
    "    ToTensorV2()\n",
    "])  \n",
    "\n",
    "# Albumentations의 val_transform (train_transform과 동일하게 구성)\n",
    "val_transform = A.Compose([\n",
    "    # 검증 시에도 동일하게 Resize 및 Normalize를 적용합니다.\n",
    "    A.LongestMaxSize(max_size=CFG['IMG_SIZE'], interpolation=Image.BILINEAR),\n",
    "    A.PadIfNeeded(min_height=CFG['IMG_SIZE'], min_width=CFG['IMG_SIZE'],\n",
    "                border_mode=0, fill=(0,0,0)), # border_mode=0 (CONSTANT), value는 패딩 색상\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225],\n",
    "                max_pixel_value=255.0),\n",
    "    ToTensorV2()    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 전체 데이터셋 로드\n",
    "full_dataset = CustomImageDataset(train_root, transform=None)\n",
    "print(f\"총 이미지 수: {len(full_dataset)}\")\n",
    "\n",
    "targets = [label for _, label in full_dataset.samples]\n",
    "class_names = full_dataset.classes\n",
    "\n",
    "# Stratified Split\n",
    "train_idx, val_idx = train_test_split(\n",
    "    range(len(targets)), test_size=0.2, stratify=targets, random_state=42\n",
    ")\n",
    "\n",
    "# Subset + transform 각각 적용  \n",
    "train_dataset = Subset(CustomImageDataset(train_root, transform=train_transform), train_idx)\n",
    "val_dataset = Subset(CustomImageDataset(train_root, transform=val_transform), val_idx)\n",
    "print(f'train 이미지 수: {len(train_dataset)}, valid 이미지 수: {len(val_dataset)}')\n",
    "\n",
    "\n",
    "# DataLoader 정의\n",
    "train_loader = DataLoader(train_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fM0RBKa5CKcr"
   },
   "source": [
    "# Model Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from src.models.tresnet_v2.tresnet_v2 import TResnetL_V2 as TResnetL368\n",
    "\n",
    "\n",
    "class TResNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(TResNet, self).__init__()\n",
    "        model_params = {'num_classes' : 196}\n",
    "        self.backbone = TResnetL368(model_params)\n",
    "        \n",
    "        weights_path = \"/kaggle/input/tresnet-stanford-cars-pretrained/stanford_cars_tresnet-l-v2_96_27.pth\"\n",
    "        pretrained_weights = torch.load(weights_path)\n",
    "        \n",
    "        self.backbone.load_state_dict(pretrained_weights['model'])  # TResnetL368 모델 불러오기\n",
    "        self.feature_dim = self.backbone.num_features\n",
    "        self.backbone.head = nn.Identity()  # feature extractor로만 사용\n",
    "        self.head = nn.Linear(self.feature_dim, num_classes)  # 분류기\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.nn.modules.batchnorm import _BatchNorm\n",
    "\n",
    "def cosine_anneal_schedule(t, nb_epoch, lr):\n",
    "    cos_inner = np.pi * (t % (nb_epoch))\n",
    "    cos_inner /= (nb_epoch)\n",
    "    cos_out = np.cos(cos_inner) + 1\n",
    "\n",
    "    return float(lr / 2 * cos_out)\n",
    "\n",
    "class BasicConv(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0, dilation=1, groups=1, relu=True, bn=True, bias=False):\n",
    "        super(BasicConv, self).__init__()\n",
    "        self.out_channels = out_planes\n",
    "        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size,\n",
    "                              stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n",
    "        self.bn = nn.BatchNorm2d(out_planes, eps=1e-5,\n",
    "                                 momentum=0.01, affine=True) if bn else None\n",
    "        self.relu = nn.ReLU() if relu else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        if self.bn is not None:\n",
    "            x = self.bn(x)\n",
    "        if self.relu is not None:\n",
    "            x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "class Features(nn.Module):\n",
    "    def __init__(self, net_layers_FeatureHead):\n",
    "        super(Features, self).__init__()\n",
    "        self.net_layer_0 = nn.Sequential(net_layers_FeatureHead[0])\n",
    "        self.net_layer_1 = nn.Sequential(*net_layers_FeatureHead[1])\n",
    "        self.net_layer_2 = nn.Sequential(*net_layers_FeatureHead[2])\n",
    "        self.net_layer_3 = nn.Sequential(*net_layers_FeatureHead[3])\n",
    "        self.net_layer_4 = nn.Sequential(*net_layers_FeatureHead[4])\n",
    "        self.net_layer_5 = nn.Sequential(*net_layers_FeatureHead[5])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net_layer_0(x)\n",
    "        x = self.net_layer_1(x)\n",
    "        x = self.net_layer_2(x)\n",
    "        x1 = self.net_layer_3(x)\n",
    "        x2 = self.net_layer_4(x1)\n",
    "        x3 = self.net_layer_5(x2)\n",
    "\n",
    "        return x1, x2, x3\n",
    "\n",
    "\n",
    "class Network_Wrapper(nn.Module):\n",
    "    def __init__(self, net_layers, num_classes, classifier):\n",
    "        super().__init__()\n",
    "        self.Features = Features(net_layers)\n",
    "        self.classifier_pool = nn.Sequential(classifier[0])\n",
    "        \n",
    "        # classifier_initial을 num_classes에 맞게 수정\n",
    "        self.classifier_initial = nn.Linear(2048, num_classes)  # 기존 196을 num_classes로 변경\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.lrelu = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n",
    "\n",
    "        self.max_pool1 = nn.MaxPool2d(kernel_size=46, stride=1)\n",
    "        self.max_pool2 = nn.MaxPool2d(kernel_size=23, stride=1)\n",
    "        self.max_pool3 = nn.MaxPool2d(kernel_size=12, stride=1)\n",
    "\n",
    "        self.conv_block1 = nn.Sequential(\n",
    "            BasicConv(512, 512, kernel_size=1, stride=1, padding=0, relu=True),\n",
    "            BasicConv(512, 1024, kernel_size=3, stride=1, padding=1, relu=True)\n",
    "        )\n",
    "        self.classifier1 = nn.Sequential(\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "        self.conv_block2 = nn.Sequential(\n",
    "            BasicConv(1024, 512, kernel_size=1, stride=1, padding=0, relu=True),\n",
    "            BasicConv(512, 1024, kernel_size=3, stride=1, padding=1, relu=True)\n",
    "        )\n",
    "        self.classifier2 = nn.Sequential(\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.Linear(512, num_classes),\n",
    "        )\n",
    "\n",
    "        self.conv_block3 = nn.Sequential(\n",
    "            BasicConv(2048, 512, kernel_size=1, stride=1, padding=0, relu=True),\n",
    "            BasicConv(512, 1024, kernel_size=3, stride=1, padding=1, relu=True)\n",
    "        )\n",
    "        self.classifier3 = nn.Sequential(\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.Linear(512, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1, x2, x3 = self.Features(x)\n",
    "        map1 = x1.clone()\n",
    "        map2 = x2.clone()\n",
    "        map3 = x3.clone()\n",
    "\n",
    "        classifiers = self.classifier_pool(x3).view(x3.size(0), -1)\n",
    "        classifiers = self.classifier_initial(classifiers)  # 이제 num_classes 출력\n",
    "\n",
    "        x1_ = self.conv_block1(x1)\n",
    "        x1_ = self.max_pool1(x1_)\n",
    "        x1_f = x1_.view(x1_.size(0), -1)\n",
    "\n",
    "        x1_c = self.classifier1(x1_f)\n",
    "\n",
    "        x2_ = self.conv_block2(x2)\n",
    "        x2_ = self.max_pool2(x2_)\n",
    "        x2_f = x2_.view(x2_.size(0), -1)\n",
    "        x2_c = self.classifier2(x2_f)\n",
    "\n",
    "        x3_ = self.conv_block3(x3)\n",
    "        x3_ = self.max_pool3(x3_)\n",
    "        x3_f = x3_.view(x3_.size(0), -1)\n",
    "        x3_c = self.classifier3(x3_f)\n",
    "\n",
    "        return x1_c, x2_c, x3_c, classifiers, map1, map2, map3\n",
    "\n",
    "\n",
    "class Anti_Noise_Decoder(nn.Module):\n",
    "    def __init__(self, scale, in_channel):\n",
    "        super(Anti_Noise_Decoder, self).__init__()\n",
    "        self.Sigmoid = nn.Sigmoid()\n",
    "\n",
    "        in_channel = in_channel // (scale * scale)\n",
    "\n",
    "        self.skip = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, 1, 1, bias=False),\n",
    "            nn.LeakyReLU(negative_slope=0.1, inplace=True),\n",
    "            nn.Conv2d(64, 3, 3, 1, 1, bias=False),\n",
    "            nn.LeakyReLU(negative_slope=0.1, inplace=True)\n",
    "\n",
    "        )\n",
    "\n",
    "        self.process = nn.Sequential(\n",
    "            nn.PixelShuffle(scale),\n",
    "            nn.Conv2d(in_channel, 256, 3, 1, 1, bias=False),\n",
    "            nn.LeakyReLU(negative_slope=0.1, inplace=True),\n",
    "            nn.PixelShuffle(2),\n",
    "            nn.Conv2d(64, 128, 3, 1, 1, bias=False),\n",
    "            nn.LeakyReLU(negative_slope=0.1, inplace=True),\n",
    "            nn.PixelShuffle(2),\n",
    "            nn.Conv2d(32, 64, 3, 1, 1, bias=False),\n",
    "            nn.LeakyReLU(negative_slope=0.1, inplace=True),\n",
    "            nn.PixelShuffle(2),\n",
    "            nn.Conv2d(16, 3, 3, 1, 1, bias=False),\n",
    "            nn.LeakyReLU(negative_slope=0.1, inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, map):\n",
    "        x_ = self.process(map)\n",
    "        if not (x.size() == x_.size()):\n",
    "            x_ = F.interpolate(x, (x.size(2),x.size(3)), mode='bilinear')\n",
    "        return self.skip(x) + x_\n",
    "\n",
    "\n",
    "def img_add_noise(x, transformation_seq):\n",
    "    x = x.permute(0, 2, 3, 1)\n",
    "    x = x.cpu().numpy()\n",
    "    x = transformation_seq(images=x)\n",
    "    x = torch.from_numpy(x.astype(np.float32))\n",
    "    x = x.permute(0, 3, 1, 2)\n",
    "    return x\n",
    "\n",
    "def smooth_crossentropy(pred, gold, smoothing=0.1):\n",
    "    n_class = pred.size(1)\n",
    "\n",
    "    one_hot = torch.full_like(pred, fill_value=smoothing / (n_class - 1))\n",
    "    one_hot.scatter_(dim=1, index=gold.unsqueeze(1), value=1.0 - smoothing)\n",
    "    log_prob = F.log_softmax(pred, dim=1)\n",
    "\n",
    "    return F.kl_div(input=log_prob, target=one_hot, reduction='none').sum(-1)\n",
    "\n",
    "def CELoss(x, y):\n",
    "    return smooth_crossentropy(x, y, smoothing=0.1)\n",
    "\n",
    "class CharbonnierLoss(nn.Module):\n",
    "    \"\"\"Charbonnier Loss (L1)\"\"\"\n",
    "\n",
    "    def __init__(self, eps=1e-3):\n",
    "        super(CharbonnierLoss, self).__init__()\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        diff = x - y\n",
    "        # loss = torch.sum(torch.sqrt(diff * diff + self.eps))\n",
    "        loss = torch.mean(torch.sqrt((diff * diff) + (self.eps*self.eps)))\n",
    "        return loss\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def disable_running_stats(model):\n",
    "    def _disable(module):\n",
    "        if isinstance(module, _BatchNorm):\n",
    "            module.backup_momentum = module.momentum\n",
    "            module.momentum = 0\n",
    "\n",
    "    model.apply(_disable)\n",
    "\n",
    "def enable_running_stats(model):\n",
    "    def _enable(module):\n",
    "        if isinstance(module, _BatchNorm) and hasattr(module, \"backup_momentum\"):\n",
    "            module.momentum = module.backup_momentum\n",
    "\n",
    "    model.apply(_enable)\n",
    "\n",
    "\n",
    "class Student_Wrapper(nn.Module):\n",
    "    def __init__(self, net_layers, classifier):\n",
    "        super(Student_Wrapper, self).__init__()\n",
    "        self.net_layer_0 = nn.Sequential(net_layers[0])\n",
    "        self.net_layer_1 = nn.Sequential(*net_layers[1])\n",
    "        self.net_layer_2 = nn.Sequential(*net_layers[2])\n",
    "        self.net_layer_3 = nn.Sequential(*net_layers[3])\n",
    "        self.net_layer_4 = nn.Sequential(*net_layers[4])\n",
    "        self.net_layer_5 = nn.Sequential(*net_layers[5])\n",
    "\n",
    "        self.classifier_pool = nn.Sequential(classifier[0])\n",
    "        self.classifier_initial = nn.Sequential(classifier[1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net_layer_0(x)\n",
    "        x = self.net_layer_1(x)\n",
    "        x = self.net_layer_2(x)\n",
    "        x1 = self.net_layer_3(x)\n",
    "        x2 = self.net_layer_4(x1)\n",
    "        x3 = self.net_layer_5(x2)\n",
    "\n",
    "\n",
    "        classifiers = self.classifier_pool(x3).view(x3.size(0), -1)\n",
    "        out = self.classifier_initial(classifiers)\n",
    "\n",
    "        return out, x1, x2, x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# SAM\n",
    "class SAM(torch.optim.Optimizer):\n",
    "    def __init__(self, params, base_optimizer, rho=0.05, adaptive=False, **kwargs):\n",
    "        assert rho >= 0.0, f\"Invalid rho, should be non-negative: {rho}\"\n",
    "\n",
    "        defaults = dict(rho=rho, adaptive=adaptive, **kwargs)\n",
    "        super(SAM, self).__init__(params, defaults)\n",
    "\n",
    "        self.base_optimizer = base_optimizer(self.param_groups, **kwargs)\n",
    "        self.param_groups = self.base_optimizer.param_groups\n",
    "        self.defaults.update(self.base_optimizer.defaults)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def first_step(self, zero_grad=False):\n",
    "        grad_norm = self._grad_norm()\n",
    "        for group in self.param_groups:\n",
    "            scale = group[\"rho\"] / (grad_norm + 1e-12)\n",
    "\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None: continue\n",
    "                self.state[p][\"old_p\"] = p.data.clone()\n",
    "                e_w = (torch.pow(p, 2) if group[\"adaptive\"] else 1.0) * p.grad * scale.to(p)\n",
    "                p.add_(e_w)  # climb to the local maximum \"w + e(w)\"\n",
    "\n",
    "        if zero_grad: self.zero_grad()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def second_step(self, zero_grad=False):\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None: continue\n",
    "                p.data = self.state[p][\"old_p\"]  # get back to \"w\" from \"w + e(w)\"\n",
    "\n",
    "        self.base_optimizer.step()  # do the actual \"sharpness-aware\" update\n",
    "\n",
    "        if zero_grad: self.zero_grad()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        assert closure is not None, \"Sharpness Aware Minimization requires closure, but it was not provided\"\n",
    "        closure = torch.enable_grad()(closure)  # the closure should do a full forward-backward pass\n",
    "\n",
    "        self.first_step(zero_grad=True)\n",
    "        closure()\n",
    "        self.second_step()\n",
    "\n",
    "    def _grad_norm(self):\n",
    "        shared_device = self.param_groups[0][\"params\"][0].device  # put everything on the same device, in case of model parallelism\n",
    "        norm = torch.norm(\n",
    "                    torch.stack([\n",
    "                        ((torch.abs(p) if group[\"adaptive\"] else 1.0) * p.grad).norm(p=2).to(shared_device)\n",
    "                        for group in self.param_groups for p in group[\"params\"]\n",
    "                        if p.grad is not None\n",
    "                    ]),\n",
    "                    p=2\n",
    "               )\n",
    "        return norm\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        super().load_state_dict(state_dict)\n",
    "        self.base_optimizer.param_groups = self.param_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Freeze weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "best_logloss = float('inf')\n",
    "\n",
    "# PMAL\n",
    "model_params = {'num_classes' : 196}\n",
    "model = TResnetL368(model_params)\n",
    "weights_path = \"/kaggle/input/tresnet-stanford-cars-pretrained/stanford_cars_tresnet-l-v2_96_27.pth\"\n",
    "pretrained_weights = torch.load(weights_path)\n",
    "model.load_state_dict(pretrained_weights['model'])\n",
    "\n",
    "net_layers = list(model.children())\n",
    "classifier = net_layers[1:3]\n",
    "net_layers = net_layers[0]\n",
    "net_layers = list(net_layers.children())\n",
    "\n",
    "# Network_Wrapper 생성\n",
    "net = Network_Wrapper(net_layers, len(class_names), classifier)\n",
    "\n",
    "# ====== Pretrained weights freeze ======\n",
    "# Features (backbone) 부분 freeze\n",
    "for param in net.Features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# # classifier_pool 부분도 freeze (기존 pretrained의 일부)\n",
    "# for param in net.classifier_pool.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "print(\"🔒 Frozen parameters:\")\n",
    "frozen_params = 0\n",
    "for name, param in net.named_parameters():\n",
    "    if not param.requires_grad:\n",
    "        frozen_params += param.numel()\n",
    "        print(f\"  - {name}: {param.shape}\")\n",
    "\n",
    "print(f\"\\n📊 Parameter Summary:\")\n",
    "total_params = sum(p.numel() for p in net.parameters())\n",
    "trainable_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Frozen parameters: {frozen_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"  Trainable ratio: {trainable_params/total_params:.2%}\")\n",
    "\n",
    "print(f\"\\n🎯 Trainable components:\")\n",
    "for name, param in net.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"  - {name}: {param.shape}\")\n",
    "\n",
    "net.to(device)\n",
    "decoder1 = Anti_Noise_Decoder(1, 512).to(device)\n",
    "decoder2 = Anti_Noise_Decoder(2, 1024).to(device)\n",
    "decoder3 = Anti_Noise_Decoder(4, 2048).to(device)\n",
    "\n",
    "#loss\n",
    "CB_loss = CharbonnierLoss()\n",
    "\n",
    "#optimizer\n",
    "base_optimizer = torch.optim.SGD\n",
    "\n",
    "optimizer = SAM([\n",
    "        {'params': net.classifier_initial.parameters(), 'lr': 0.002},\n",
    "        {'params': net.conv_block1.parameters(), 'lr': 0.002},\n",
    "        {'params': net.classifier1.parameters(), 'lr': 0.002},\n",
    "        {'params': net.conv_block2.parameters(), 'lr': 0.002},\n",
    "        {'params': net.classifier2.parameters(), 'lr': 0.002},\n",
    "        {'params': net.conv_block3.parameters(), 'lr': 0.002},\n",
    "        {'params': net.classifier3.parameters(), 'lr': 0.002},\n",
    "\n",
    "        {'params': decoder1.skip.parameters(), 'lr': 0.002},\n",
    "        {'params': decoder1.process.parameters(), 'lr': 0.002},\n",
    "        {'params': decoder2.skip.parameters(), 'lr': 0.002},\n",
    "        {'params': decoder2.process.parameters(), 'lr': 0.002},\n",
    "        {'params': decoder3.skip.parameters(), 'lr': 0.002},\n",
    "        {'params': decoder3.process.parameters(), 'lr': 0.002},\n",
    "\n",
    "    ],\n",
    "        base_optimizer, adaptive=False, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "max_val_acc = 0\n",
    "lr = [0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ====== 선택적 unfreezing (optional) ======\n",
    "def unfreeze_last_n_blocks(model, n_blocks=1):\n",
    "    \"\"\"마지막 n개 블록만 unfreeze (fine-tuning 시 사용)\"\"\"\n",
    "    # TResNet의 경우 body의 마지막 몇 개 layer만 unfreeze\n",
    "    if hasattr(model.Features, 'body'):\n",
    "        body_layers = list(model.Features.body.children())\n",
    "        for layer in body_layers[-n_blocks:]:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = True\n",
    "    print(f\"🔓 Unfroze last {n_blocks} blocks\")\n",
    "\n",
    "def unfreeze_classifier_pool():\n",
    "    \"\"\"classifier_pool도 학습하고 싶다면\"\"\"\n",
    "    for param in net.classifier_pool.parameters():\n",
    "        param.requires_grad = True\n",
    "    print(\"🔓 Unfroze classifier_pool\")\n",
    "\n",
    "# 사용 예시 (필요시 주석 해제):\n",
    "# unfreeze_last_n_blocks(net, n_blocks=1)  # 마지막 1개 블록 unfreeze\n",
    "# unfreeze_classifier_pool()  # classifier_pool unfreeze\n",
    "\n",
    "# ====== 학습률 스케줄링 (optional) ======\n",
    "def get_parameter_groups_with_different_lr(model, backbone_lr=1e-5, new_layers_lr=1e-3):\n",
    "    \"\"\"backbone과 새로운 layer에 다른 학습률 적용\"\"\"\n",
    "    backbone_params = []\n",
    "    new_layer_params = []\n",
    "    \n",
    "    # Backbone (frozen이 아닌 경우)\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            if 'Features' in name:\n",
    "                backbone_params.append(param)\n",
    "            else:\n",
    "                new_layer_params.append(param)\n",
    "    \n",
    "    return [\n",
    "        {'params': backbone_params, 'lr': backbone_lr},\n",
    "        {'params': new_layer_params, 'lr': new_layers_lr}\n",
    "    ]\n",
    "\n",
    "# 다른 학습률 사용하고 싶다면:\n",
    "# param_groups = get_parameter_groups_with_different_lr(net, backbone_lr=1e-5, new_layers_lr=1e-3)\n",
    "# optimizer = SAM(param_groups, torch.optim.SGD, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# best_logloss = float('inf')\n",
    "\n",
    "# # # 손실 함수\n",
    "# # criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# # # 옵티마이저\n",
    "# # optimizer = optim.Adam(model.parameters(), lr=CFG['LEARNING_RATE'])\n",
    "\n",
    "\n",
    "# # PMAL\n",
    "# model_params = {'num_classes' : 196}\n",
    "# model = TResnetL368(model_params)\n",
    "# weights_path = \"/kaggle/input/tresnet-stanford-cars-pretrained/stanford_cars_tresnet-l-v2_96_27.pth\"\n",
    "# pretrained_weights = torch.load(weights_path)\n",
    "# model.load_state_dict(pretrained_weights['model'])\n",
    "\n",
    "# net_layers = list(model.children())\n",
    "# classifier = net_layers[1:3]\n",
    "# net_layers = net_layers[0]\n",
    "# net_layers = list(net_layers.children())\n",
    "\n",
    "# net = Network_Wrapper(net_layers, len(class_names), classifier)\n",
    "# # netp = torch.nn.DataParallel(net, device_ids=[0])\n",
    "\n",
    "# net.to(device)\n",
    "# decoder1 = Anti_Noise_Decoder(1, 512).to(device)\n",
    "# decoder2 = Anti_Noise_Decoder(2, 1024).to(device)\n",
    "# decoder3 = Anti_Noise_Decoder(4, 2048).to(device)\n",
    "\n",
    "# #loss\n",
    "# CB_loss = CharbonnierLoss()\n",
    "\n",
    "# #optimizer\n",
    "# base_optimizer = torch.optim.SGD\n",
    "\n",
    "# optimizer = SAM([\n",
    "#         {'params': net.classifier_initial.parameters(), 'lr': 0.002},\n",
    "#         {'params': net.conv_block1.parameters(), 'lr': 0.002},\n",
    "#         {'params': net.classifier1.parameters(), 'lr': 0.002},\n",
    "#         {'params': net.conv_block2.parameters(), 'lr': 0.002},\n",
    "#         {'params': net.classifier2.parameters(), 'lr': 0.002},\n",
    "#         {'params': net.conv_block3.parameters(), 'lr': 0.002},\n",
    "#         {'params': net.classifier3.parameters(), 'lr': 0.002},\n",
    "\n",
    "#         {'params': decoder1.skip.parameters(), 'lr': 0.002},\n",
    "#         {'params': decoder1.process.parameters(), 'lr': 0.002},\n",
    "#         {'params': decoder2.skip.parameters(), 'lr': 0.002},\n",
    "#         {'params': decoder2.process.parameters(), 'lr': 0.002},\n",
    "#         {'params': decoder3.skip.parameters(), 'lr': 0.002},\n",
    "#         {'params': decoder3.process.parameters(), 'lr': 0.002},\n",
    "\n",
    "#         {'params': net.Features.parameters(), 'lr': 0.0002}\n",
    "\n",
    "#     ],\n",
    "#         base_optimizer, adaptive=False, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "# max_val_acc = 0\n",
    "# lr = [0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.0002]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NGlK2nPYCKcr"
   },
   "source": [
    "# Train/ Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 522
    },
    "executionInfo": {
     "elapsed": 263293,
     "status": "error",
     "timestamp": 1747893498648,
     "user": {
      "displayName": "박진영",
      "userId": "15299328254354703813"
     },
     "user_tz": -540
    },
    "id": "DwNWkTC3CKcr",
    "outputId": "c3e4f7fa-70b2-4225-bcc9-b185a00f9f99",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "gradient_accumulation = CFG['gradient_accumulation']\n",
    "\n",
    "if not gradient_accumulation:\n",
    "    \n",
    "    # 학습 및 검증 루프\n",
    "    for epoch in range(CFG['EPOCHS']):\n",
    "        # Train\n",
    "        net.train()\n",
    "        train_loss = 0.0\n",
    "        train_loss1 = 0\n",
    "        train_loss2 = 0\n",
    "        train_loss3 = 0\n",
    "        train_loss4 = 0\n",
    "        train_loss5 = 0\n",
    "    \n",
    "        for images, targets in tqdm(train_loader, desc=f\"[Epoch {epoch+1}/{CFG['EPOCHS']}] Training\"):\n",
    "            images, targets = images.to(device), targets.to(device)\n",
    "    \n",
    "            for nlr in range(len(optimizer.param_groups)):\n",
    "                optimizer.param_groups[nlr]['lr'] = cosine_anneal_schedule(epoch, CFG['EPOCHS'], lr[nlr])\n",
    "            \n",
    "            sometimes_1 = lambda aug: iaa.Sometimes(0.2, aug)\n",
    "            sometimes_2 = lambda aug: iaa.Sometimes(0.5, aug)\n",
    "    \n",
    "            trans_seq_aug = iaa.Sequential(\n",
    "                [\n",
    "    \n",
    "                    sometimes_1(iaa.Affine(\n",
    "                        scale={\"x\": (0.8, 1.2), \"y\": (0.8, 1.2)},\n",
    "                        translate_percent={\"x\": (-0.2, 0.2), \"y\": (-0.2, 0.2)},\n",
    "                        rotate=(-15, 15),\n",
    "                        shear=(-15, 15),\n",
    "                        order=[0, 1],\n",
    "                        cval=(0, 1),\n",
    "                        mode=ia.ALL\n",
    "                    )),\n",
    "                    sometimes_2(iaa.GaussianBlur((0, 3.0)))\n",
    "                ],\n",
    "                random_order=True\n",
    "            )\n",
    "    \n",
    "            trans_seq = iaa.Sequential(\n",
    "                [\n",
    "    \n",
    "                    iaa.AdditiveGaussianNoise(\n",
    "                        loc=0, scale=(0.0, 0.05), per_channel=0.5\n",
    "                    )\n",
    "                ],\n",
    "                random_order=True\n",
    "            )\n",
    "            # H1 first forward-backward step\n",
    "            enable_running_stats(net)\n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "            inputs1_gt = img_add_noise(images, trans_seq_aug).to(device)\n",
    "            inputs1 = img_add_noise(inputs1_gt, trans_seq).to(device)\n",
    "            output_1, _, _, _, map1, _, _ = net(inputs1)\n",
    "    \n",
    "            loss1_c = CELoss(output_1, targets).mean() * 1\n",
    "            inputs1_syn = decoder1(inputs1, map1)\n",
    "            loss1_g = CB_loss(inputs1_syn, inputs1_gt) * 1\n",
    "    \n",
    "            output_1_syn, _, _, _, _, _, _ = net(inputs1_syn)\n",
    "            loss1_c_syn = CELoss(output_1_syn, targets).mean() * 1\n",
    "    \n",
    "            loss1 = loss1_c + (loss1_g) + loss1_c_syn\n",
    "            loss1.backward()\n",
    "            optimizer.first_step(zero_grad=True)\n",
    "    \n",
    "            # H1 second forward-backward step\n",
    "            disable_running_stats(net)\n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "            output_1, _, _, _, map1, _, _ = net(inputs1)\n",
    "            loss1_c = CELoss(output_1, targets).mean() * 1\n",
    "    \n",
    "            inputs1_syn = decoder1(inputs1, map1)\n",
    "            loss1_g = CB_loss(inputs1_syn, inputs1_gt) * 1\n",
    "    \n",
    "            output_1_syn, _, _, _, _, _, _ = net(inputs1_syn)\n",
    "            loss1_c_syn = CELoss(output_1_syn, targets).mean() * 1\n",
    "    \n",
    "            loss1_ = loss1_c + loss1_g + loss1_c_syn\n",
    "            loss1_.backward()\n",
    "            optimizer.second_step(zero_grad=True)\n",
    "    \n",
    "            loss1 = loss1.cpu()\n",
    "            loss1_g = loss1_g.cpu()\n",
    "    \n",
    "            del output_1\n",
    "            del output_1_syn\n",
    "            del loss1_\n",
    "            del loss1_c\n",
    "            del loss1_c_syn\n",
    "            del inputs1\n",
    "            del inputs1_gt\n",
    "            del inputs1_syn\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "            # H2\n",
    "            # H2 first forward-backward step\n",
    "            enable_running_stats(net)\n",
    "            optimizer.zero_grad()\n",
    "            inputs2_gt = img_add_noise(images, trans_seq_aug).to(device)\n",
    "            inputs2 = img_add_noise(inputs2_gt, trans_seq).to(device)\n",
    "            _, output_2, _, _, _, map2, _ = net(inputs2)\n",
    "            loss2_c = CELoss(output_2, targets).mean() * 1\n",
    "    \n",
    "            inputs2_syn = decoder2(inputs2, map2)\n",
    "            loss2_g = CB_loss(inputs2_syn, inputs2_gt) * 1\n",
    "    \n",
    "            _, output_2_syn, _, _, _, _, _ = net(inputs2_syn)\n",
    "            loss2_c_syn = CELoss(output_2_syn, targets).mean() * 1\n",
    "    \n",
    "            loss2 = loss2_c + loss2_g + loss2_c_syn\n",
    "            loss2.backward()\n",
    "            optimizer.first_step(zero_grad=True)\n",
    "    \n",
    "            # H2 second forward-backward step\n",
    "            disable_running_stats(net)\n",
    "            optimizer.zero_grad()\n",
    "            _, output_2, _, _, _, map2, _ = net(inputs2)\n",
    "            loss2_c = CELoss(output_2, targets).mean() * 1\n",
    "    \n",
    "            inputs2_syn = decoder2(inputs2, map2)\n",
    "            loss2_g = CB_loss(inputs2_syn, inputs2_gt) * 1\n",
    "    \n",
    "            _, output_2_syn, _, _, _, _, _ = net(inputs2_syn)\n",
    "            loss2_c_syn = CELoss(output_2_syn, targets).mean() * 1\n",
    "    \n",
    "            loss2_ = loss2_c + (loss2_g) + loss2_c_syn\n",
    "            loss2_.backward()\n",
    "            optimizer.second_step(zero_grad=True)\n",
    "    \n",
    "            loss2 = loss2.cpu()\n",
    "            loss2_g = loss2_g.cpu()\n",
    "            del output_2\n",
    "            del output_2_syn\n",
    "            del loss2_\n",
    "            del loss2_c\n",
    "            del loss2_c_syn\n",
    "            del inputs2\n",
    "            del inputs2_gt\n",
    "            del inputs2_syn\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "            # H3\n",
    "            # H3 first forward-backward step\n",
    "            enable_running_stats(net)\n",
    "            optimizer.zero_grad()\n",
    "            inputs3_gt = img_add_noise(images, trans_seq_aug).to(device)\n",
    "            inputs3 = img_add_noise(inputs3_gt, trans_seq).to(device)\n",
    "            _, _, output_3, _, _, _, map3 = net(inputs3)\n",
    "            loss3_c = CELoss(output_3, targets).mean() * 1\n",
    "    \n",
    "            inputs3_syn = decoder3(inputs3, map3)\n",
    "            loss3_g = CB_loss(inputs3_syn, inputs3_gt) * 1\n",
    "    \n",
    "            _, _, output_3_syn, _, _, _, _ = net(inputs3_syn)\n",
    "            loss3_c_syn = CELoss(output_3_syn, targets).mean() * 1\n",
    "    \n",
    "            loss3 = loss3_c + (loss3_g) + loss3_c_syn\n",
    "            loss3.backward()\n",
    "            optimizer.first_step(zero_grad=True)\n",
    "    \n",
    "            # H3 second forward-backward step\n",
    "            disable_running_stats(net)\n",
    "            optimizer.zero_grad()\n",
    "            _, _, output_3, _, _, _, map3 = net(inputs3)\n",
    "            loss3_c = CELoss(output_3, targets).mean() * 1\n",
    "    \n",
    "            inputs3_syn = decoder3(inputs3, map3)\n",
    "            loss3_g = CB_loss(inputs3_syn, inputs3_gt) * 1\n",
    "    \n",
    "            _, _, output_3_syn, _, _, _, _ = net(inputs3_syn)\n",
    "            loss3_c_syn = CELoss(output_3_syn, targets).mean() * 1\n",
    "    \n",
    "            loss3_ = loss3_c + (loss3_g) + loss3_c_syn\n",
    "            loss3_.backward()\n",
    "            optimizer.second_step(zero_grad=True)\n",
    "    \n",
    "            loss3 = loss3.cpu()\n",
    "            loss3_g = loss3_g.cpu()\n",
    "            del output_3\n",
    "            del output_3_syn\n",
    "            del loss3_\n",
    "            del loss3_c\n",
    "            del loss3_c_syn\n",
    "            del inputs3\n",
    "            del inputs3_gt\n",
    "            del inputs3_syn\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "            # H4\n",
    "            # H4 first forward-backward step\n",
    "            enable_running_stats(net)\n",
    "            optimizer.zero_grad()\n",
    "            output_1_final, output_2_final, output_3_final, output_ORI, _, _, _ = net(images)\n",
    "            ORI_loss = CELoss(output_1_final, targets).mean() + \\\n",
    "                        CELoss(output_2_final, targets).mean() + \\\n",
    "                        CELoss(output_3_final, targets).mean() + \\\n",
    "                        CELoss(output_ORI, targets).mean() * 2\n",
    "            # 손실 계산 전에 targets 검사\n",
    "    \n",
    "            ORI_loss.backward()\n",
    "            optimizer.first_step(zero_grad=True)\n",
    "    \n",
    "            # H4 second forward-backward step\n",
    "            disable_running_stats(net)\n",
    "            optimizer.zero_grad()\n",
    "            output_1_final, output_2_final, output_3_final, output_ORI, _, _, _ = net(images)\n",
    "            ORI_loss_ = CELoss(output_1_final, targets).mean() + \\\n",
    "                        CELoss(output_2_final, targets).mean() + \\\n",
    "                        CELoss(output_3_final, targets).mean() + \\\n",
    "                        CELoss(output_ORI, targets).mean() * 2\n",
    "            ORI_loss_.backward()\n",
    "            optimizer.second_step(zero_grad=True)\n",
    "    \n",
    "            ORI_loss = ORI_loss.cpu()\n",
    "            del output_1_final\n",
    "            del output_2_final\n",
    "            del output_3_final\n",
    "            output_ORI = output_ORI.cpu()\n",
    "            targets = targets.cpu()\n",
    "            del images\n",
    "            del ORI_loss_\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "            train_loss += (loss1.item() + loss2.item() + loss3.item() + ORI_loss.item()) # \n",
    "            train_loss1 += loss1.item()\n",
    "            train_loss2 += loss2.item()\n",
    "            train_loss3 += loss3.item()\n",
    "            train_loss4 += (loss1_g.item() + loss2_g.item() + loss3_g.item())\n",
    "            train_loss5 += ORI_loss.item()\n",
    "        total = len(train_loader)\n",
    "        avg_train_loss = train_loss / total\n",
    "        avg_train_loss1 = train_loss1 / total\n",
    "        avg_train_loss2 = train_loss2 / total\n",
    "        avg_train_loss3 = train_loss3 / total\n",
    "        avg_train_loss4 = train_loss4 / total\n",
    "        avg_train_loss5 = train_loss5 / total\n",
    "    \n",
    "        # Validation\n",
    "        net.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        correct_com = 0\n",
    "        total = 0\n",
    "        all_probs = []\n",
    "        all_probs_com = []\n",
    "        all_labels = []\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            for images, labels in tqdm(val_loader, desc=f\"[Epoch {epoch+1}/{CFG['EPOCHS']}] Validation\"):\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "    \n",
    "                output_1, output_2, output_3, output_ORI, _, _, _ = net(images)\n",
    "    \n",
    "                outputs_com = output_1.cpu() + output_2.cpu() + output_3.cpu() + output_ORI.cpu()\n",
    "                loss = CELoss(output_ORI, labels)\n",
    "                val_loss += loss.item()\n",
    "    \n",
    "                # Accuracy\n",
    "                _, preds = torch.max(output_ORI, 1)\n",
    "                _, preds_com = torch.max(outputs_com, 1)\n",
    "    \n",
    "                correct += (preds == labels).sum().item()\n",
    "                correct_com += (preds_com == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "    \n",
    "                # LogLoss\n",
    "                probs = F.softmax(output_ORI, dim=1)\n",
    "                all_probs.extend(probs.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "                probs_com = F.softmax(outputs_com, dim=1)\n",
    "                all_probs_com.extend(probs_com.cpu().numpy())\n",
    "    \n",
    "    \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_accuracy = 100 * correct / total\n",
    "        val_com_accuracy = 100 * correct_com / total\n",
    "        val_logloss = log_loss(all_labels, all_probs, labels=list(range(len(class_names))))\n",
    "        val_com_logloss = log_loss(all_labels, all_probs_com, labels=list(range(len(class_names))))\n",
    "        del images\n",
    "        del loss\n",
    "        del targets\n",
    "        del output_1\n",
    "        del output_2\n",
    "        del output_3\n",
    "        del output_ORI\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # wandb \n",
    "        wandb.log({\n",
    "            \"train_loss\": avg_train_loss,\n",
    "            \"train_loss1\": avg_train_loss1,\n",
    "            \"train_loss2\": avg_train_loss2,\n",
    "            \"train_loss3\": avg_train_loss3,\n",
    "            \"train_loss4\": avg_train_loss4,\n",
    "            \"train_loss5\": avg_train_loss5,\n",
    "            \"val_loss\": avg_val_loss,\n",
    "            \"val_accuracy\": val_accuracy,\n",
    "            \"val_com_accuracy\": val_com_accuracy,\n",
    "            \"val_logloss\": val_logloss,\n",
    "            \"val_com_logloss\": val_com_logloss,\n",
    "        })\n",
    "        \n",
    "        # 결과 출력\n",
    "        print(f\"Train Loss : {avg_train_loss:.4f} || Valid Loss : {avg_val_loss:.4f} | Valid Accuracy : {val_accuracy:.4f}%\")\n",
    "    \n",
    "        # Best model 저장\n",
    "        if val_logloss < best_logloss:\n",
    "            best_logloss = val_logloss\n",
    "            torch.save(net.state_dict(), f'best_model.pth')\n",
    "            print(f\"📦 Best model saved at epoch {epoch+1} (logloss: {val_logloss:.4f})\")\n",
    "            torch.save(decoder1, f'decoder1.pth')\n",
    "            torch.save(decoder1, f'decoder2.pth')\n",
    "            torch.save(decoder1, f'decoder3.pth')\n",
    "    \n",
    "    \n",
    "        early_stopping(val_logloss, model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(f\"🛑 Early stopping triggered at epoch {epoch+1}\")\n",
    "            break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Gradient Accumulation 설정\n",
    "if gradient_accumulation:\n",
    "\n",
    "    accumulation_steps = CFG['accumulation_steps']  # 원하는 accumulation step 수로 조정\n",
    "    \n",
    "    # 학습 및 검증 루프\n",
    "    for epoch in range(CFG['EPOCHS']):\n",
    "        # Train\n",
    "        net.train()\n",
    "        train_loss = 0.0\n",
    "        train_loss1 = 0\n",
    "        train_loss2 = 0\n",
    "        train_loss3 = 0\n",
    "        train_loss4 = 0\n",
    "        train_loss5 = 0\n",
    "        \n",
    "        # Gradient accumulation을 위한 변수들\n",
    "        accumulated_loss1 = 0\n",
    "        accumulated_loss2 = 0\n",
    "        accumulated_loss3 = 0\n",
    "        accumulated_loss4 = 0\n",
    "    \n",
    "        for batch_idx, (images, targets) in enumerate(tqdm(train_loader, desc=f\"[Epoch {epoch+1}/{CFG['EPOCHS']}] Training\")):\n",
    "            images, targets = images.to(device), targets.to(device)\n",
    "    \n",
    "            for nlr in range(len(optimizer.param_groups)):\n",
    "                optimizer.param_groups[nlr]['lr'] = cosine_anneal_schedule(epoch, CFG['EPOCHS'], lr[nlr])\n",
    "            \n",
    "            sometimes_1 = lambda aug: iaa.Sometimes(0.2, aug)\n",
    "            sometimes_2 = lambda aug: iaa.Sometimes(0.5, aug)\n",
    "    \n",
    "            trans_seq_aug = iaa.Sequential(\n",
    "                [\n",
    "                    sometimes_1(iaa.Affine(\n",
    "                        scale={\"x\": (0.8, 1.2), \"y\": (0.8, 1.2)},\n",
    "                        translate_percent={\"x\": (-0.2, 0.2), \"y\": (-0.2, 0.2)},\n",
    "                        rotate=(-15, 15),\n",
    "                        shear=(-15, 15),\n",
    "                        order=[0, 1],\n",
    "                        cval=(0, 1),\n",
    "                        mode=ia.ALL\n",
    "                    )),\n",
    "                    sometimes_2(iaa.GaussianBlur((0, 3.0)))\n",
    "                ],\n",
    "                random_order=True\n",
    "            )\n",
    "    \n",
    "            trans_seq = iaa.Sequential(\n",
    "                [\n",
    "                    iaa.AdditiveGaussianNoise(\n",
    "                        loc=0, scale=(0.0, 0.05), per_channel=0.5\n",
    "                    )\n",
    "                ],\n",
    "                random_order=True\n",
    "            )\n",
    "            \n",
    "            # H1 first forward-backward step\n",
    "            enable_running_stats(net)\n",
    "            if batch_idx % accumulation_steps == 0:\n",
    "                optimizer.zero_grad()\n",
    "    \n",
    "            inputs1_gt = img_add_noise(images, trans_seq_aug).to(device)\n",
    "            inputs1 = img_add_noise(inputs1_gt, trans_seq).to(device)\n",
    "            output_1, _, _, _, map1, _, _ = net(inputs1)\n",
    "    \n",
    "            loss1_c = CELoss(output_1, targets).mean() * 1\n",
    "            inputs1_syn = decoder1(inputs1, map1)\n",
    "            loss1_g = CB_loss(inputs1_syn, inputs1_gt) * 1\n",
    "    \n",
    "            output_1_syn, _, _, _, _, _, _ = net(inputs1_syn)\n",
    "            loss1_c_syn = CELoss(output_1_syn, targets).mean() * 1\n",
    "    \n",
    "            loss1 = (loss1_c + loss1_g + loss1_c_syn) / accumulation_steps  # accumulation으로 나누기\n",
    "            loss1.backward()\n",
    "            \n",
    "            accumulated_loss1 += loss1.item() * accumulation_steps  # 실제 loss 값 저장\n",
    "            \n",
    "            if (batch_idx + 1) % accumulation_steps == 0 or (batch_idx + 1) == len(train_loader):\n",
    "                optimizer.first_step(zero_grad=True)\n",
    "    \n",
    "            # H1 second forward-backward step\n",
    "            disable_running_stats(net)\n",
    "            if batch_idx % accumulation_steps == 0:\n",
    "                optimizer.zero_grad()\n",
    "    \n",
    "            output_1, _, _, _, map1, _, _ = net(inputs1)\n",
    "            loss1_c = CELoss(output_1, targets).mean() * 1\n",
    "    \n",
    "            inputs1_syn = decoder1(inputs1, map1)\n",
    "            loss1_g = CB_loss(inputs1_syn, inputs1_gt) * 1\n",
    "    \n",
    "            output_1_syn, _, _, _, _, _, _ = net(inputs1_syn)\n",
    "            loss1_c_syn = CELoss(output_1_syn, targets).mean() * 1\n",
    "    \n",
    "            loss1_ = (loss1_c + loss1_g + loss1_c_syn) / accumulation_steps\n",
    "            loss1_.backward()\n",
    "            \n",
    "            if (batch_idx + 1) % accumulation_steps == 0 or (batch_idx + 1) == len(train_loader):\n",
    "                optimizer.second_step(zero_grad=True)\n",
    "    \n",
    "            loss1_g_cpu = loss1_g.cpu()\n",
    "    \n",
    "            del output_1, output_1_syn, loss1_, loss1_c, loss1_c_syn\n",
    "            del inputs1, inputs1_gt, inputs1_syn\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "            # H2 first forward-backward step\n",
    "            enable_running_stats(net)\n",
    "            if batch_idx % accumulation_steps == 0:\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "            inputs2_gt = img_add_noise(images, trans_seq_aug).to(device)\n",
    "            inputs2 = img_add_noise(inputs2_gt, trans_seq).to(device)\n",
    "            _, output_2, _, _, _, map2, _ = net(inputs2)\n",
    "            loss2_c = CELoss(output_2, targets).mean() * 1\n",
    "    \n",
    "            inputs2_syn = decoder2(inputs2, map2)\n",
    "            loss2_g = CB_loss(inputs2_syn, inputs2_gt) * 1\n",
    "    \n",
    "            _, output_2_syn, _, _, _, _, _ = net(inputs2_syn)\n",
    "            loss2_c_syn = CELoss(output_2_syn, targets).mean() * 1\n",
    "    \n",
    "            loss2 = (loss2_c + loss2_g + loss2_c_syn) / accumulation_steps\n",
    "            loss2.backward()\n",
    "            \n",
    "            accumulated_loss2 += loss2.item() * accumulation_steps\n",
    "            \n",
    "            if (batch_idx + 1) % accumulation_steps == 0 or (batch_idx + 1) == len(train_loader):\n",
    "                optimizer.first_step(zero_grad=True)\n",
    "    \n",
    "            # H2 second forward-backward step\n",
    "            disable_running_stats(net)\n",
    "            if batch_idx % accumulation_steps == 0:\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "            _, output_2, _, _, _, map2, _ = net(inputs2)\n",
    "            loss2_c = CELoss(output_2, targets).mean() * 1\n",
    "    \n",
    "            inputs2_syn = decoder2(inputs2, map2)\n",
    "            loss2_g = CB_loss(inputs2_syn, inputs2_gt) * 1\n",
    "    \n",
    "            _, output_2_syn, _, _, _, _, _ = net(inputs2_syn)\n",
    "            loss2_c_syn = CELoss(output_2_syn, targets).mean() * 1\n",
    "    \n",
    "            loss2_ = (loss2_c + loss2_g + loss2_c_syn) / accumulation_steps\n",
    "            loss2_.backward()\n",
    "            \n",
    "            if (batch_idx + 1) % accumulation_steps == 0 or (batch_idx + 1) == len(train_loader):\n",
    "                optimizer.second_step(zero_grad=True)\n",
    "    \n",
    "            loss2_g_cpu = loss2_g.cpu()\n",
    "            \n",
    "            del output_2, output_2_syn, loss2_, loss2_c, loss2_c_syn\n",
    "            del inputs2, inputs2_gt, inputs2_syn\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "            # H3 first forward-backward step\n",
    "            enable_running_stats(net)\n",
    "            if batch_idx % accumulation_steps == 0:\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "            inputs3_gt = img_add_noise(images, trans_seq_aug).to(device)\n",
    "            inputs3 = img_add_noise(inputs3_gt, trans_seq).to(device)\n",
    "            _, _, output_3, _, _, _, map3 = net(inputs3)\n",
    "            loss3_c = CELoss(output_3, targets).mean() * 1\n",
    "    \n",
    "            inputs3_syn = decoder3(inputs3, map3)\n",
    "            loss3_g = CB_loss(inputs3_syn, inputs3_gt) * 1\n",
    "    \n",
    "            _, _, output_3_syn, _, _, _, _ = net(inputs3_syn)\n",
    "            loss3_c_syn = CELoss(output_3_syn, targets).mean() * 1\n",
    "    \n",
    "            loss3 = (loss3_c + loss3_g + loss3_c_syn) / accumulation_steps\n",
    "            loss3.backward()\n",
    "            \n",
    "            accumulated_loss3 += loss3.item() * accumulation_steps\n",
    "            \n",
    "            if (batch_idx + 1) % accumulation_steps == 0 or (batch_idx + 1) == len(train_loader):\n",
    "                optimizer.first_step(zero_grad=True)\n",
    "    \n",
    "            # H3 second forward-backward step\n",
    "            disable_running_stats(net)\n",
    "            if batch_idx % accumulation_steps == 0:\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "            _, _, output_3, _, _, _, map3 = net(inputs3)\n",
    "            loss3_c = CELoss(output_3, targets).mean() * 1\n",
    "    \n",
    "            inputs3_syn = decoder3(inputs3, map3)\n",
    "            loss3_g = CB_loss(inputs3_syn, inputs3_gt) * 1\n",
    "    \n",
    "            _, _, output_3_syn, _, _, _, _ = net(inputs3_syn)\n",
    "            loss3_c_syn = CELoss(output_3_syn, targets).mean() * 1\n",
    "    \n",
    "            loss3_ = (loss3_c + loss3_g + loss3_c_syn) / accumulation_steps\n",
    "            loss3_.backward()\n",
    "            \n",
    "            if (batch_idx + 1) % accumulation_steps == 0 or (batch_idx + 1) == len(train_loader):\n",
    "                optimizer.second_step(zero_grad=True)\n",
    "    \n",
    "            loss3_g_cpu = loss3_g.cpu()\n",
    "            \n",
    "            del output_3, output_3_syn, loss3_, loss3_c, loss3_c_syn\n",
    "            del inputs3, inputs3_gt, inputs3_syn\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "            # H4 first forward-backward step\n",
    "            enable_running_stats(net)\n",
    "            if batch_idx % accumulation_steps == 0:\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "            output_1_final, output_2_final, output_3_final, output_ORI, _, _, _ = net(images)\n",
    "            ORI_loss = (CELoss(output_1_final, targets).mean() + \\\n",
    "                       CELoss(output_2_final, targets).mean() + \\\n",
    "                       CELoss(output_3_final, targets).mean() + \\\n",
    "                       CELoss(output_ORI, targets).mean() * 2) / accumulation_steps\n",
    "    \n",
    "            ORI_loss.backward()\n",
    "            \n",
    "            accumulated_loss4 += ORI_loss.item() * accumulation_steps\n",
    "            \n",
    "            if (batch_idx + 1) % accumulation_steps == 0 or (batch_idx + 1) == len(train_loader):\n",
    "                optimizer.first_step(zero_grad=True)\n",
    "    \n",
    "            # H4 second forward-backward step\n",
    "            disable_running_stats(net)\n",
    "            if batch_idx % accumulation_steps == 0:\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "            output_1_final, output_2_final, output_3_final, output_ORI, _, _, _ = net(images)\n",
    "            ORI_loss_ = (CELoss(output_1_final, targets).mean() + \\\n",
    "                        CELoss(output_2_final, targets).mean() + \\\n",
    "                        CELoss(output_3_final, targets).mean() + \\\n",
    "                        CELoss(output_ORI, targets).mean() * 2) / accumulation_steps\n",
    "            ORI_loss_.backward()\n",
    "            \n",
    "            if (batch_idx + 1) % accumulation_steps == 0 or (batch_idx + 1) == len(train_loader):\n",
    "                optimizer.second_step(zero_grad=True)\n",
    "    \n",
    "            del output_1_final, output_2_final, output_3_final, output_ORI\n",
    "            del images, targets, ORI_loss_\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "            # accumulation step이 완료되었을 때만 loss 누적\n",
    "            if (batch_idx + 1) % accumulation_steps == 0 or (batch_idx + 1) == len(train_loader):\n",
    "                train_loss += (accumulated_loss1 + accumulated_loss2 + accumulated_loss3 + accumulated_loss4)\n",
    "                train_loss1 += accumulated_loss1\n",
    "                train_loss2 += accumulated_loss2\n",
    "                train_loss3 += accumulated_loss3\n",
    "                train_loss4 += (loss1_g_cpu.item() + loss2_g_cpu.item() + loss3_g_cpu.item())\n",
    "                train_loss5 += accumulated_loss4\n",
    "                \n",
    "                # 누적 변수 초기화\n",
    "                accumulated_loss1 = 0\n",
    "                accumulated_loss2 = 0\n",
    "                accumulated_loss3 = 0\n",
    "                accumulated_loss4 = 0\n",
    "    \n",
    "        # 평균 계산 시 실제 step 수로 나누기\n",
    "        actual_steps = (len(train_loader) + accumulation_steps - 1) // accumulation_steps\n",
    "        avg_train_loss = train_loss / actual_steps\n",
    "        avg_train_loss1 = train_loss1 / actual_steps\n",
    "        avg_train_loss2 = train_loss2 / actual_steps\n",
    "        avg_train_loss3 = train_loss3 / actual_steps\n",
    "        avg_train_loss4 = train_loss4 / actual_steps\n",
    "        avg_train_loss5 = train_loss5 / actual_steps\n",
    "    \n",
    "        # Validation (기존과 동일)\n",
    "        net.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        correct_com = 0\n",
    "        total = 0\n",
    "        all_probs = []\n",
    "        all_probs_com = []\n",
    "        all_labels = []\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            for images, labels in tqdm(val_loader, desc=f\"[Epoch {epoch+1}/{CFG['EPOCHS']}] Validation\"):\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "    \n",
    "                output_1, output_2, output_3, output_ORI, _, _, _ = net(images)\n",
    "    \n",
    "                outputs_com = output_1.cpu() + output_2.cpu() + output_3.cpu() + output_ORI.cpu()\n",
    "                loss = CELoss(output_ORI, labels)\n",
    "                val_loss += loss.item()\n",
    "    \n",
    "                # Accuracy\n",
    "                _, preds = torch.max(output_ORI, 1)\n",
    "                _, preds_com = torch.max(outputs_com, 1)\n",
    "    \n",
    "                correct += (preds == labels).sum().item()\n",
    "                correct_com += (preds_com == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "    \n",
    "                # LogLoss\n",
    "                probs = F.softmax(output_ORI, dim=1)\n",
    "                all_probs.extend(probs.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "                probs_com = F.softmax(outputs_com, dim=1)\n",
    "                all_probs_com.extend(probs_com.cpu().numpy())\n",
    "    \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_accuracy = 100 * correct / total\n",
    "        val_com_accuracy = 100 * correct_com / total\n",
    "        val_logloss = log_loss(all_labels, all_probs, labels=list(range(len(class_names))))\n",
    "        val_com_logloss = log_loss(all_labels, all_probs_com, labels=list(range(len(class_names))))\n",
    "        \n",
    "        del images, loss, output_1, output_2, output_3, output_ORI\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # wandb \n",
    "        wandb.log({\n",
    "            \"train_loss\": avg_train_loss,\n",
    "            \"train_loss1\": avg_train_loss1,\n",
    "            \"train_loss2\": avg_train_loss2,\n",
    "            \"train_loss3\": avg_train_loss3,\n",
    "            \"train_loss4\": avg_train_loss4,\n",
    "            \"train_loss5\": avg_train_loss5,\n",
    "            \"val_loss\": avg_val_loss,\n",
    "            \"val_accuracy\": val_accuracy,\n",
    "            \"val_com_accuracy\": val_com_accuracy,\n",
    "            \"val_logloss\": val_logloss,\n",
    "            \"val_com_logloss\": val_com_logloss,\n",
    "        })\n",
    "        \n",
    "        # 결과 출력\n",
    "        print(f\"Train Loss : {avg_train_loss:.4f} || Valid Loss : {avg_val_loss:.4f} | Valid Accuracy : {val_accuracy:.4f}%\")\n",
    "    \n",
    "        # Best model 저장\n",
    "        if val_logloss < best_logloss:\n",
    "            best_logloss = val_logloss\n",
    "            torch.save(net.state_dict(), f'best_model.pth')\n",
    "            print(f\"📦 Best model saved at epoch {epoch+1} (logloss: {val_logloss:.4f})\")\n",
    "            torch.save(decoder1, f'decoder1.pth')\n",
    "            torch.save(decoder2, f'decoder2.pth')  # decoder2로 수정\n",
    "            torch.save(decoder3, f'decoder3.pth')  # decoder3으로 수정\n",
    "    \n",
    "        early_stopping(val_logloss, model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(f\"🛑 Early stopping triggered at epoch {epoch+1}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TwGAOAiKCKcr"
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 379040,
     "status": "aborted",
     "timestamp": 1747893498644,
     "user": {
      "displayName": "박진영",
      "userId": "15299328254354703813"
     },
     "user_tz": -540
    },
    "id": "4vS6URwRCKcr",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_dataset = CustomImageDataset(test_root, transform=val_transform, is_test=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 379037,
     "status": "aborted",
     "timestamp": 1747893498646,
     "user": {
      "displayName": "박진영",
      "userId": "15299328254354703813"
     },
     "user_tz": -540
    },
    "id": "i8XWppr-CKcr",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 저장된 모델 로드\n",
    "model = TResnetL368(model_params)\n",
    "model.load_state_dict(torch.load('best_model.pth', map_location=device))\n",
    "net_layers = list(model.children())\n",
    "classifier = net_layers[1:3]\n",
    "net_layers = net_layers[0]\n",
    "net_layers = list(net_layers.children())\n",
    "net = Network_Wrapper(net_layers, len(class_names), classifier)\n",
    "net.to(device)\n",
    "# 추론\n",
    "net.eval()\n",
    "results = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images in test_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = net(images)\n",
    "        probs = F.softmax(outputs, dim=1)\n",
    "\n",
    "        # 각 배치의 확률을 리스트로 변환\n",
    "        for prob in probs.cpu():  # prob: (num_classes,)\n",
    "            result = {\n",
    "                class_names[i]: prob[i].item()\n",
    "                for i in range(len(class_names))\n",
    "            }\n",
    "            results.append(result)\n",
    "\n",
    "pred = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HKj-nq9RCKcr"
   },
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 379034,
     "status": "aborted",
     "timestamp": 1747893498647,
     "user": {
      "displayName": "박진영",
      "userId": "15299328254354703813"
     },
     "user_tz": -540
    },
    "id": "9VcLATLfCKcr",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "submission = pd.read_csv('/kaggle/input/car-classification/sample_submission.csv', encoding='utf-8-sig')\n",
    "\n",
    "# 'ID' 컬럼을 제외한 클래스 컬럼 정렬\n",
    "class_columns = submission.columns[1:]\n",
    "pred = pred[class_columns]\n",
    "\n",
    "submission[class_columns] = pred.values\n",
    "submission.to_csv('baseline_submission.csv', index=False, encoding='utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 1053211,
     "sourceId": 1771962,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7483280,
     "sourceId": 11904337,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7492523,
     "sourceId": 11918208,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
