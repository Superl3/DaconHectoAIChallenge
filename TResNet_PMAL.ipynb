{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L3oV47WeCKcn"
   },
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11180,
     "status": "ok",
     "timestamp": 1747893157421,
     "user": {
      "displayName": "Î∞ïÏßÑÏòÅ",
      "userId": "15299328254354703813"
     },
     "user_tz": -540
    },
    "id": "5VRIs0HCCKco",
    "outputId": "81c5c51f-7a68-430b-a8df-7ec278f2fa4b",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install inplace-abn\n",
    "!pip install imgaug\n",
    "!pip install early_stopping_pytorch\n",
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import imgaug as ia\n",
    "import imgaug.augmenters as iaa\n",
    "from early_stopping_pytorch import EarlyStopping\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/Alibaba-MIIL/TResNet\n",
    "%cd TResNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6hXgRhYKCKcp"
   },
   "source": [
    "# Hyperparameter Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1747893157442,
     "user": {
      "displayName": "Î∞ïÏßÑÏòÅ",
      "userId": "15299328254354703813"
     },
     "user_tz": -540
    },
    "id": "xvRCnuRqCKcp",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'IMG_SIZE': 368,\n",
    "    'BATCH_SIZE': 16,\n",
    "    'EPOCHS': 20,\n",
    "    'LEARNING_RATE': 1e-4,\n",
    "    'SEED' : 42,\n",
    "    'gradient_accumulation' : True,\n",
    "    'accumulation_steps' : 4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!wandb login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Initialize wandb\n",
    "wandb.init(\n",
    "    entity='Dacon_Car',\n",
    "    project=\"car-classification\",  # your project name\n",
    "    name='TResNet_PMAL',\n",
    "    config=CFG  # this will log your hyperparameters\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ullOr-KjCKcp"
   },
   "source": [
    "# Fixed RandomSeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 32,
     "status": "ok",
     "timestamp": 1747893157484,
     "user": {
      "displayName": "Î∞ïÏßÑÏòÅ",
      "userId": "15299328254354703813"
     },
     "user_tz": -540
    },
    "id": "Uwop2i4qCKcp",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)    \n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(CFG['SEED']) # Seed Í≥†Ï†ï"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AUwzuA7jCKcq"
   },
   "source": [
    "# CustomDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 31,
     "status": "ok",
     "timestamp": 1747893157488,
     "user": {
      "displayName": "Î∞ïÏßÑÏòÅ",
      "userId": "15299328254354703813"
     },
     "user_tz": -540
    },
    "id": "EOA2BdsbCKcq",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np # NumPy ÏûÑÌè¨Ìä∏ Ï∂îÍ∞Ä\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, is_test=False):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.is_test = is_test\n",
    "        self.samples = []\n",
    "\n",
    "        if is_test:\n",
    "            # ÌÖåÏä§Ìä∏ÏÖã: ÎùºÎ≤® ÏóÜÏù¥ Ïù¥ÎØ∏ÏßÄ Í≤ΩÎ°úÎßå Ï†ÄÏû•\n",
    "            for fname in sorted(os.listdir(root_dir)):\n",
    "                if fname.lower().endswith(('.jpg', '.jpeg', '.png', '.gif')): # Ïù¥ÎØ∏ÏßÄ ÌôïÏû•Ïûê Ï∂îÍ∞Ä\n",
    "                    img_path = os.path.join(root_dir, fname)\n",
    "                    self.samples.append((img_path,))\n",
    "        else:\n",
    "            # ÌïôÏäµÏÖã: ÌÅ¥ÎûòÏä§Î≥Ñ Ìè¥Îçî Íµ¨Ï°∞ÏóêÏÑú ÎùºÎ≤® Ï∂îÏ∂ú\n",
    "            self.classes = sorted(os.listdir(root_dir))\n",
    "            self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}\n",
    "\n",
    "            for cls_name in self.classes:\n",
    "                cls_folder = os.path.join(root_dir, cls_name)\n",
    "                # Ìè¥ÎçîÍ∞Ä ÏïÑÎãå ÌååÏùºÏù¥ ÏûàÏùÑ Ïàò ÏûàÏúºÎØÄÎ°ú isdir Ï≤¥ÌÅ¨ Ï∂îÍ∞Ä\n",
    "                if not os.path.isdir(cls_folder):\n",
    "                    continue\n",
    "                for fname in os.listdir(cls_folder):\n",
    "                    if fname.lower().endswith(('.jpg', '.jpeg', '.png', '.gif')): # Ïù¥ÎØ∏ÏßÄ ÌôïÏû•Ïûê Ï∂îÍ∞Ä\n",
    "                        img_path = os.path.join(cls_folder, fname)\n",
    "                        label = self.class_to_idx[cls_name]\n",
    "                        self.samples.append((img_path, label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.is_test:\n",
    "            img_path = self.samples[idx][0]\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            # PIL Ïù¥ÎØ∏ÏßÄÎ•º NumPy Î∞∞Ïó¥Î°ú Î≥ÄÌôò\n",
    "            image = np.array(image)\n",
    "\n",
    "            if self.transform:\n",
    "                # AlbumentationsÎäî ÎîïÏÖîÎÑàÎ¶¨Î•º Î∞òÌôòÌïòÎ©∞ 'image' ÌÇ§Ïóê Î≥ÄÌôòÎêú Ïù¥ÎØ∏ÏßÄÍ∞Ä ÏûàÏäµÎãàÎã§.\n",
    "                transformed_data = self.transform(image=image)\n",
    "                image = transformed_data['image'] # PyTorch ÌÖêÏÑú (C, H, W)\n",
    "\n",
    "            return image\n",
    "        else:\n",
    "            img_path, label = self.samples[idx]\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            # PIL Ïù¥ÎØ∏ÏßÄÎ•º NumPy Î∞∞Ïó¥Î°ú Î≥ÄÌôò\n",
    "            image = np.array(image)\n",
    "\n",
    "            if self.transform:\n",
    "                # AlbumentationsÎäî ÎîïÏÖîÎÑàÎ¶¨Î•º Î∞òÌôòÌïòÎ©∞ 'image' ÌÇ§Ïóê Î≥ÄÌôòÎêú Ïù¥ÎØ∏ÏßÄÍ∞Ä ÏûàÏäµÎãàÎã§.\n",
    "                transformed_data = self.transform(image=image)\n",
    "                image = transformed_data['image'] # PyTorch ÌÖêÏÑú (C, H, W)\n",
    "\n",
    "            return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6bmib4EdCKcq"
   },
   "source": [
    "# Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1747893157492,
     "user": {
      "displayName": "Î∞ïÏßÑÏòÅ",
      "userId": "15299328254354703813"
     },
     "user_tz": -540
    },
    "id": "GLNBNSyDCKcq",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_root = '/kaggle/input/car-classification/train'\n",
    "test_root = '/kaggle/input/car-classification/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2 # PyTorch ÌÖêÏÑúÎ°ú Î≥ÄÌôòÌïòÍ∏∞ ÏúÑÌï®\n",
    "import numpy as np # AlbumentationsÎäî NumPy Î∞∞Ïó¥ÏùÑ ÏûÖÎ†•ÏúºÎ°ú Î∞õÏäµÎãàÎã§.\n",
    "from PIL import Image # Ïù¥ÎØ∏ÏßÄ Î°úÎî©ÏùÑ ÏúÑÌïú ÎùºÏù¥Î∏åÎü¨Î¶¨\n",
    "\n",
    "# AlbumentationsÏùò train_transform\n",
    "train_transform = A.Compose([\n",
    "    # ResizeIfPadNeededÎäî Í∞ÄÎ°úÏÑ∏Î°ú ÎπÑÏú®ÏùÑ Ïú†ÏßÄÌïòÎ©¥ÏÑú Ïù¥ÎØ∏ÏßÄÏùò Í∏¥ Î≥Ä ÎòêÎäî ÏßßÏùÄ Î≥ÄÏùÑ Î¶¨ÏÇ¨Ïù¥Ï¶àÌïú Îã§Ïùå,\n",
    "    # ÏßÄÏ†ïÎêú ÌÅ¨Í∏∞Ïóê ÎßûÏ∂∞ Ìå®Îî©ÏùÑ Ï∂îÍ∞ÄÌï©ÎãàÎã§.\n",
    "    # pad_height, pad_widthÎäî ÏµúÏ¢Ö Ï∂úÎ†• ÌÅ¨Í∏∞Î•º ÏùòÎØ∏Ìï©ÎãàÎã§.\n",
    "    # ÎßåÏïΩ ÏõêÎ≥∏ ÎπÑÏú®ÏùÑ Ïú†ÏßÄÌïòÎ©¥ÏÑú Ìå®Îî©ÏúºÎ°ú Ï±ÑÏö∞Îäî Í≤ÉÏù¥ Î™©Ï†ÅÏù¥ÎùºÎ©¥ ÏïÑÎûòÏôÄ Í∞ôÏù¥ LongestMaxSizeÏôÄ PadIfNeededÎ•º ÏÇ¨Ïö©Ìï©ÎãàÎã§.\n",
    "    A.LongestMaxSize(max_size=CFG['IMG_SIZE'], interpolation=Image.BILINEAR),\n",
    "    A.PadIfNeeded(min_height=CFG['IMG_SIZE'], min_width=CFG['IMG_SIZE'],\n",
    "                border_mode=0, fill=(0,0,0)), # border_mode=0 (CONSTANT), valueÎäî Ìå®Îî© ÏÉâÏÉÅ\n",
    "\n",
    "    # ÏùºÎ∞òÏ†ÅÏúºÎ°ú ÌïôÏäµ ÏãúÏóêÎäî Resize ÌõÑ NormalizeÎ•º ÎßéÏù¥ ÏÇ¨Ïö©Ìï©ÎãàÎã§.\n",
    "    # torchvisionÏùò NormalizeÏôÄ ÎèôÏùºÌïú mean/std Í∞íÏùÑ ÏÇ¨Ïö©Ìï©ÎãàÎã§.\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225],\n",
    "                max_pixel_value=255.0), # Ïù¥ÎØ∏ÏßÄ ÌîΩÏÖÄ Í∞íÏùò ÏµúÎåìÍ∞í (ÏùºÎ∞òÏ†ÅÏúºÎ°ú 255)\n",
    "\n",
    "    # AlbumentationsÏùò ToTensorV2Îäî Ïù¥ÎØ∏ÏßÄÎ•º PyTorch ÌÖêÏÑúÎ°ú Î≥ÄÌôòÌïòÍ≥† Ï±ÑÎÑê ÏàúÏÑúÎ•º (H, W, C) -> (C, H, W)Î°ú Î≥ÄÍ≤ΩÌï©ÎãàÎã§.\n",
    "    # torchvisionÏùò ToTensor()ÏôÄ Ïú†ÏÇ¨ÌïòÍ≤å ÎèôÏûëÌï©ÎãàÎã§.\n",
    "    ToTensorV2()\n",
    "])  \n",
    "\n",
    "# AlbumentationsÏùò val_transform (train_transformÍ≥º ÎèôÏùºÌïòÍ≤å Íµ¨ÏÑ±)\n",
    "val_transform = A.Compose([\n",
    "    # Í≤ÄÏ¶ù ÏãúÏóêÎèÑ ÎèôÏùºÌïòÍ≤å Resize Î∞è NormalizeÎ•º Ï†ÅÏö©Ìï©ÎãàÎã§.\n",
    "    A.LongestMaxSize(max_size=CFG['IMG_SIZE'], interpolation=Image.BILINEAR),\n",
    "    A.PadIfNeeded(min_height=CFG['IMG_SIZE'], min_width=CFG['IMG_SIZE'],\n",
    "                border_mode=0, fill=(0,0,0)), # border_mode=0 (CONSTANT), valueÎäî Ìå®Îî© ÏÉâÏÉÅ\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225],\n",
    "                max_pixel_value=255.0),\n",
    "    ToTensorV2()    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Ï†ÑÏ≤¥ Îç∞Ïù¥ÌÑ∞ÏÖã Î°úÎìú\n",
    "full_dataset = CustomImageDataset(train_root, transform=None)\n",
    "print(f\"Ï¥ù Ïù¥ÎØ∏ÏßÄ Ïàò: {len(full_dataset)}\")\n",
    "\n",
    "targets = [label for _, label in full_dataset.samples]\n",
    "class_names = full_dataset.classes\n",
    "\n",
    "# Stratified Split\n",
    "train_idx, val_idx = train_test_split(\n",
    "    range(len(targets)), test_size=0.2, stratify=targets, random_state=42\n",
    ")\n",
    "\n",
    "# Subset + transform Í∞ÅÍ∞Å Ï†ÅÏö©  \n",
    "train_dataset = Subset(CustomImageDataset(train_root, transform=train_transform), train_idx)\n",
    "val_dataset = Subset(CustomImageDataset(train_root, transform=val_transform), val_idx)\n",
    "print(f'train Ïù¥ÎØ∏ÏßÄ Ïàò: {len(train_dataset)}, valid Ïù¥ÎØ∏ÏßÄ Ïàò: {len(val_dataset)}')\n",
    "\n",
    "\n",
    "# DataLoader Ï†ïÏùò\n",
    "train_loader = DataLoader(train_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fM0RBKa5CKcr"
   },
   "source": [
    "# Model Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from src.models.tresnet_v2.tresnet_v2 import TResnetL_V2 as TResnetL368\n",
    "\n",
    "\n",
    "class TResNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(TResNet, self).__init__()\n",
    "        model_params = {'num_classes' : 196}\n",
    "        self.backbone = TResnetL368(model_params)\n",
    "        \n",
    "        weights_path = \"/kaggle/input/tresnet-stanford-cars-pretrained/stanford_cars_tresnet-l-v2_96_27.pth\"\n",
    "        pretrained_weights = torch.load(weights_path)\n",
    "        \n",
    "        self.backbone.load_state_dict(pretrained_weights['model'])  # TResnetL368 Î™®Îç∏ Î∂àÎü¨Ïò§Í∏∞\n",
    "        self.feature_dim = self.backbone.num_features\n",
    "        self.backbone.head = nn.Identity()  # feature extractorÎ°úÎßå ÏÇ¨Ïö©\n",
    "        self.head = nn.Linear(self.feature_dim, num_classes)  # Î∂ÑÎ•òÍ∏∞\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.nn.modules.batchnorm import _BatchNorm\n",
    "\n",
    "def cosine_anneal_schedule(t, nb_epoch, lr):\n",
    "    cos_inner = np.pi * (t % (nb_epoch))\n",
    "    cos_inner /= (nb_epoch)\n",
    "    cos_out = np.cos(cos_inner) + 1\n",
    "\n",
    "    return float(lr / 2 * cos_out)\n",
    "\n",
    "class BasicConv(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0, dilation=1, groups=1, relu=True, bn=True, bias=False):\n",
    "        super(BasicConv, self).__init__()\n",
    "        self.out_channels = out_planes\n",
    "        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size,\n",
    "                              stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n",
    "        self.bn = nn.BatchNorm2d(out_planes, eps=1e-5,\n",
    "                                 momentum=0.01, affine=True) if bn else None\n",
    "        self.relu = nn.ReLU() if relu else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        if self.bn is not None:\n",
    "            x = self.bn(x)\n",
    "        if self.relu is not None:\n",
    "            x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "class Features(nn.Module):\n",
    "    def __init__(self, net_layers_FeatureHead):\n",
    "        super(Features, self).__init__()\n",
    "        self.net_layer_0 = nn.Sequential(net_layers_FeatureHead[0])\n",
    "        self.net_layer_1 = nn.Sequential(*net_layers_FeatureHead[1])\n",
    "        self.net_layer_2 = nn.Sequential(*net_layers_FeatureHead[2])\n",
    "        self.net_layer_3 = nn.Sequential(*net_layers_FeatureHead[3])\n",
    "        self.net_layer_4 = nn.Sequential(*net_layers_FeatureHead[4])\n",
    "        self.net_layer_5 = nn.Sequential(*net_layers_FeatureHead[5])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net_layer_0(x)\n",
    "        x = self.net_layer_1(x)\n",
    "        x = self.net_layer_2(x)\n",
    "        x1 = self.net_layer_3(x)\n",
    "        x2 = self.net_layer_4(x1)\n",
    "        x3 = self.net_layer_5(x2)\n",
    "\n",
    "        return x1, x2, x3\n",
    "\n",
    "\n",
    "class Network_Wrapper(nn.Module):\n",
    "    def __init__(self, net_layers, num_classes, classifier):\n",
    "        super().__init__()\n",
    "        self.Features = Features(net_layers)\n",
    "        self.classifier_pool = nn.Sequential(classifier[0])\n",
    "        \n",
    "        # classifier_initialÏùÑ num_classesÏóê ÎßûÍ≤å ÏàòÏ†ï\n",
    "        self.classifier_initial = nn.Linear(2048, num_classes)  # Í∏∞Ï°¥ 196ÏùÑ num_classesÎ°ú Î≥ÄÍ≤Ω\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.lrelu = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n",
    "\n",
    "        self.max_pool1 = nn.MaxPool2d(kernel_size=46, stride=1)\n",
    "        self.max_pool2 = nn.MaxPool2d(kernel_size=23, stride=1)\n",
    "        self.max_pool3 = nn.MaxPool2d(kernel_size=12, stride=1)\n",
    "\n",
    "        self.conv_block1 = nn.Sequential(\n",
    "            BasicConv(512, 512, kernel_size=1, stride=1, padding=0, relu=True),\n",
    "            BasicConv(512, 1024, kernel_size=3, stride=1, padding=1, relu=True)\n",
    "        )\n",
    "        self.classifier1 = nn.Sequential(\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "        self.conv_block2 = nn.Sequential(\n",
    "            BasicConv(1024, 512, kernel_size=1, stride=1, padding=0, relu=True),\n",
    "            BasicConv(512, 1024, kernel_size=3, stride=1, padding=1, relu=True)\n",
    "        )\n",
    "        self.classifier2 = nn.Sequential(\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.Linear(512, num_classes),\n",
    "        )\n",
    "\n",
    "        self.conv_block3 = nn.Sequential(\n",
    "            BasicConv(2048, 512, kernel_size=1, stride=1, padding=0, relu=True),\n",
    "            BasicConv(512, 1024, kernel_size=3, stride=1, padding=1, relu=True)\n",
    "        )\n",
    "        self.classifier3 = nn.Sequential(\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.Linear(512, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1, x2, x3 = self.Features(x)\n",
    "        map1 = x1.clone()\n",
    "        map2 = x2.clone()\n",
    "        map3 = x3.clone()\n",
    "\n",
    "        classifiers = self.classifier_pool(x3).view(x3.size(0), -1)\n",
    "        classifiers = self.classifier_initial(classifiers)  # Ïù¥Ï†ú num_classes Ï∂úÎ†•\n",
    "\n",
    "        x1_ = self.conv_block1(x1)\n",
    "        x1_ = self.max_pool1(x1_)\n",
    "        x1_f = x1_.view(x1_.size(0), -1)\n",
    "\n",
    "        x1_c = self.classifier1(x1_f)\n",
    "\n",
    "        x2_ = self.conv_block2(x2)\n",
    "        x2_ = self.max_pool2(x2_)\n",
    "        x2_f = x2_.view(x2_.size(0), -1)\n",
    "        x2_c = self.classifier2(x2_f)\n",
    "\n",
    "        x3_ = self.conv_block3(x3)\n",
    "        x3_ = self.max_pool3(x3_)\n",
    "        x3_f = x3_.view(x3_.size(0), -1)\n",
    "        x3_c = self.classifier3(x3_f)\n",
    "\n",
    "        return x1_c, x2_c, x3_c, classifiers, map1, map2, map3\n",
    "\n",
    "\n",
    "class Anti_Noise_Decoder(nn.Module):\n",
    "    def __init__(self, scale, in_channel):\n",
    "        super(Anti_Noise_Decoder, self).__init__()\n",
    "        self.Sigmoid = nn.Sigmoid()\n",
    "\n",
    "        in_channel = in_channel // (scale * scale)\n",
    "\n",
    "        self.skip = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, 1, 1, bias=False),\n",
    "            nn.LeakyReLU(negative_slope=0.1, inplace=True),\n",
    "            nn.Conv2d(64, 3, 3, 1, 1, bias=False),\n",
    "            nn.LeakyReLU(negative_slope=0.1, inplace=True)\n",
    "\n",
    "        )\n",
    "\n",
    "        self.process = nn.Sequential(\n",
    "            nn.PixelShuffle(scale),\n",
    "            nn.Conv2d(in_channel, 256, 3, 1, 1, bias=False),\n",
    "            nn.LeakyReLU(negative_slope=0.1, inplace=True),\n",
    "            nn.PixelShuffle(2),\n",
    "            nn.Conv2d(64, 128, 3, 1, 1, bias=False),\n",
    "            nn.LeakyReLU(negative_slope=0.1, inplace=True),\n",
    "            nn.PixelShuffle(2),\n",
    "            nn.Conv2d(32, 64, 3, 1, 1, bias=False),\n",
    "            nn.LeakyReLU(negative_slope=0.1, inplace=True),\n",
    "            nn.PixelShuffle(2),\n",
    "            nn.Conv2d(16, 3, 3, 1, 1, bias=False),\n",
    "            nn.LeakyReLU(negative_slope=0.1, inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, map):\n",
    "        x_ = self.process(map)\n",
    "        if not (x.size() == x_.size()):\n",
    "            x_ = F.interpolate(x, (x.size(2),x.size(3)), mode='bilinear')\n",
    "        return self.skip(x) + x_\n",
    "\n",
    "\n",
    "def img_add_noise(x, transformation_seq):\n",
    "    x = x.permute(0, 2, 3, 1)\n",
    "    x = x.cpu().numpy()\n",
    "    x = transformation_seq(images=x)\n",
    "    x = torch.from_numpy(x.astype(np.float32))\n",
    "    x = x.permute(0, 3, 1, 2)\n",
    "    return x\n",
    "\n",
    "def smooth_crossentropy(pred, gold, smoothing=0.1):\n",
    "    n_class = pred.size(1)\n",
    "\n",
    "    one_hot = torch.full_like(pred, fill_value=smoothing / (n_class - 1))\n",
    "    one_hot.scatter_(dim=1, index=gold.unsqueeze(1), value=1.0 - smoothing)\n",
    "    log_prob = F.log_softmax(pred, dim=1)\n",
    "\n",
    "    return F.kl_div(input=log_prob, target=one_hot, reduction='none').sum(-1)\n",
    "\n",
    "def CELoss(x, y):\n",
    "    return smooth_crossentropy(x, y, smoothing=0.1)\n",
    "\n",
    "class CharbonnierLoss(nn.Module):\n",
    "    \"\"\"Charbonnier Loss (L1)\"\"\"\n",
    "\n",
    "    def __init__(self, eps=1e-3):\n",
    "        super(CharbonnierLoss, self).__init__()\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        diff = x - y\n",
    "        # loss = torch.sum(torch.sqrt(diff * diff + self.eps))\n",
    "        loss = torch.mean(torch.sqrt((diff * diff) + (self.eps*self.eps)))\n",
    "        return loss\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def disable_running_stats(model):\n",
    "    def _disable(module):\n",
    "        if isinstance(module, _BatchNorm):\n",
    "            module.backup_momentum = module.momentum\n",
    "            module.momentum = 0\n",
    "\n",
    "    model.apply(_disable)\n",
    "\n",
    "def enable_running_stats(model):\n",
    "    def _enable(module):\n",
    "        if isinstance(module, _BatchNorm) and hasattr(module, \"backup_momentum\"):\n",
    "            module.momentum = module.backup_momentum\n",
    "\n",
    "    model.apply(_enable)\n",
    "\n",
    "\n",
    "class Student_Wrapper(nn.Module):\n",
    "    def __init__(self, net_layers, classifier):\n",
    "        super(Student_Wrapper, self).__init__()\n",
    "        self.net_layer_0 = nn.Sequential(net_layers[0])\n",
    "        self.net_layer_1 = nn.Sequential(*net_layers[1])\n",
    "        self.net_layer_2 = nn.Sequential(*net_layers[2])\n",
    "        self.net_layer_3 = nn.Sequential(*net_layers[3])\n",
    "        self.net_layer_4 = nn.Sequential(*net_layers[4])\n",
    "        self.net_layer_5 = nn.Sequential(*net_layers[5])\n",
    "\n",
    "        self.classifier_pool = nn.Sequential(classifier[0])\n",
    "        self.classifier_initial = nn.Sequential(classifier[1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net_layer_0(x)\n",
    "        x = self.net_layer_1(x)\n",
    "        x = self.net_layer_2(x)\n",
    "        x1 = self.net_layer_3(x)\n",
    "        x2 = self.net_layer_4(x1)\n",
    "        x3 = self.net_layer_5(x2)\n",
    "\n",
    "\n",
    "        classifiers = self.classifier_pool(x3).view(x3.size(0), -1)\n",
    "        out = self.classifier_initial(classifiers)\n",
    "\n",
    "        return out, x1, x2, x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# SAM\n",
    "class SAM(torch.optim.Optimizer):\n",
    "    def __init__(self, params, base_optimizer, rho=0.05, adaptive=False, **kwargs):\n",
    "        assert rho >= 0.0, f\"Invalid rho, should be non-negative: {rho}\"\n",
    "\n",
    "        defaults = dict(rho=rho, adaptive=adaptive, **kwargs)\n",
    "        super(SAM, self).__init__(params, defaults)\n",
    "\n",
    "        self.base_optimizer = base_optimizer(self.param_groups, **kwargs)\n",
    "        self.param_groups = self.base_optimizer.param_groups\n",
    "        self.defaults.update(self.base_optimizer.defaults)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def first_step(self, zero_grad=False):\n",
    "        grad_norm = self._grad_norm()\n",
    "        for group in self.param_groups:\n",
    "            scale = group[\"rho\"] / (grad_norm + 1e-12)\n",
    "\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None: continue\n",
    "                self.state[p][\"old_p\"] = p.data.clone()\n",
    "                e_w = (torch.pow(p, 2) if group[\"adaptive\"] else 1.0) * p.grad * scale.to(p)\n",
    "                p.add_(e_w)  # climb to the local maximum \"w + e(w)\"\n",
    "\n",
    "        if zero_grad: self.zero_grad()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def second_step(self, zero_grad=False):\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None: continue\n",
    "                p.data = self.state[p][\"old_p\"]  # get back to \"w\" from \"w + e(w)\"\n",
    "\n",
    "        self.base_optimizer.step()  # do the actual \"sharpness-aware\" update\n",
    "\n",
    "        if zero_grad: self.zero_grad()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        assert closure is not None, \"Sharpness Aware Minimization requires closure, but it was not provided\"\n",
    "        closure = torch.enable_grad()(closure)  # the closure should do a full forward-backward pass\n",
    "\n",
    "        self.first_step(zero_grad=True)\n",
    "        closure()\n",
    "        self.second_step()\n",
    "\n",
    "    def _grad_norm(self):\n",
    "        shared_device = self.param_groups[0][\"params\"][0].device  # put everything on the same device, in case of model parallelism\n",
    "        norm = torch.norm(\n",
    "                    torch.stack([\n",
    "                        ((torch.abs(p) if group[\"adaptive\"] else 1.0) * p.grad).norm(p=2).to(shared_device)\n",
    "                        for group in self.param_groups for p in group[\"params\"]\n",
    "                        if p.grad is not None\n",
    "                    ]),\n",
    "                    p=2\n",
    "               )\n",
    "        return norm\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        super().load_state_dict(state_dict)\n",
    "        self.base_optimizer.param_groups = self.param_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Freeze weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "best_logloss = float('inf')\n",
    "\n",
    "# PMAL\n",
    "model_params = {'num_classes' : 196}\n",
    "model = TResnetL368(model_params)\n",
    "weights_path = \"/kaggle/input/tresnet-stanford-cars-pretrained/stanford_cars_tresnet-l-v2_96_27.pth\"\n",
    "pretrained_weights = torch.load(weights_path)\n",
    "model.load_state_dict(pretrained_weights['model'])\n",
    "\n",
    "net_layers = list(model.children())\n",
    "classifier = net_layers[1:3]\n",
    "net_layers = net_layers[0]\n",
    "net_layers = list(net_layers.children())\n",
    "\n",
    "# Network_Wrapper ÏÉùÏÑ±\n",
    "net = Network_Wrapper(net_layers, len(class_names), classifier)\n",
    "\n",
    "# ====== Pretrained weights freeze ======\n",
    "# Features (backbone) Î∂ÄÎ∂Ñ freeze\n",
    "for param in net.Features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# # classifier_pool Î∂ÄÎ∂ÑÎèÑ freeze (Í∏∞Ï°¥ pretrainedÏùò ÏùºÎ∂Ä)\n",
    "# for param in net.classifier_pool.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "print(\"üîí Frozen parameters:\")\n",
    "frozen_params = 0\n",
    "for name, param in net.named_parameters():\n",
    "    if not param.requires_grad:\n",
    "        frozen_params += param.numel()\n",
    "        print(f\"  - {name}: {param.shape}\")\n",
    "\n",
    "print(f\"\\nüìä Parameter Summary:\")\n",
    "total_params = sum(p.numel() for p in net.parameters())\n",
    "trainable_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Frozen parameters: {frozen_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"  Trainable ratio: {trainable_params/total_params:.2%}\")\n",
    "\n",
    "print(f\"\\nüéØ Trainable components:\")\n",
    "for name, param in net.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"  - {name}: {param.shape}\")\n",
    "\n",
    "net.to(device)\n",
    "decoder1 = Anti_Noise_Decoder(1, 512).to(device)\n",
    "decoder2 = Anti_Noise_Decoder(2, 1024).to(device)\n",
    "decoder3 = Anti_Noise_Decoder(4, 2048).to(device)\n",
    "\n",
    "#loss\n",
    "CB_loss = CharbonnierLoss()\n",
    "\n",
    "#optimizer\n",
    "base_optimizer = torch.optim.SGD\n",
    "\n",
    "optimizer = SAM([\n",
    "        {'params': net.classifier_initial.parameters(), 'lr': 0.002},\n",
    "        {'params': net.conv_block1.parameters(), 'lr': 0.002},\n",
    "        {'params': net.classifier1.parameters(), 'lr': 0.002},\n",
    "        {'params': net.conv_block2.parameters(), 'lr': 0.002},\n",
    "        {'params': net.classifier2.parameters(), 'lr': 0.002},\n",
    "        {'params': net.conv_block3.parameters(), 'lr': 0.002},\n",
    "        {'params': net.classifier3.parameters(), 'lr': 0.002},\n",
    "\n",
    "        {'params': decoder1.skip.parameters(), 'lr': 0.002},\n",
    "        {'params': decoder1.process.parameters(), 'lr': 0.002},\n",
    "        {'params': decoder2.skip.parameters(), 'lr': 0.002},\n",
    "        {'params': decoder2.process.parameters(), 'lr': 0.002},\n",
    "        {'params': decoder3.skip.parameters(), 'lr': 0.002},\n",
    "        {'params': decoder3.process.parameters(), 'lr': 0.002},\n",
    "\n",
    "    ],\n",
    "        base_optimizer, adaptive=False, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "max_val_acc = 0\n",
    "lr = [0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ====== ÏÑ†ÌÉùÏ†Å unfreezing (optional) ======\n",
    "def unfreeze_last_n_blocks(model, n_blocks=1):\n",
    "    \"\"\"ÎßàÏßÄÎßâ nÍ∞ú Î∏îÎ°ùÎßå unfreeze (fine-tuning Ïãú ÏÇ¨Ïö©)\"\"\"\n",
    "    # TResNetÏùò Í≤ΩÏö∞ bodyÏùò ÎßàÏßÄÎßâ Î™á Í∞ú layerÎßå unfreeze\n",
    "    if hasattr(model.Features, 'body'):\n",
    "        body_layers = list(model.Features.body.children())\n",
    "        for layer in body_layers[-n_blocks:]:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = True\n",
    "    print(f\"üîì Unfroze last {n_blocks} blocks\")\n",
    "\n",
    "def unfreeze_classifier_pool():\n",
    "    \"\"\"classifier_poolÎèÑ ÌïôÏäµÌïòÍ≥† Ïã∂Îã§Î©¥\"\"\"\n",
    "    for param in net.classifier_pool.parameters():\n",
    "        param.requires_grad = True\n",
    "    print(\"üîì Unfroze classifier_pool\")\n",
    "\n",
    "# ÏÇ¨Ïö© ÏòàÏãú (ÌïÑÏöîÏãú Ï£ºÏÑù Ìï¥Ï†ú):\n",
    "# unfreeze_last_n_blocks(net, n_blocks=1)  # ÎßàÏßÄÎßâ 1Í∞ú Î∏îÎ°ù unfreeze\n",
    "# unfreeze_classifier_pool()  # classifier_pool unfreeze\n",
    "\n",
    "# ====== ÌïôÏäµÎ•† Ïä§ÏºÄÏ§ÑÎßÅ (optional) ======\n",
    "def get_parameter_groups_with_different_lr(model, backbone_lr=1e-5, new_layers_lr=1e-3):\n",
    "    \"\"\"backboneÍ≥º ÏÉàÎ°úÏö¥ layerÏóê Îã§Î•∏ ÌïôÏäµÎ•† Ï†ÅÏö©\"\"\"\n",
    "    backbone_params = []\n",
    "    new_layer_params = []\n",
    "    \n",
    "    # Backbone (frozenÏù¥ ÏïÑÎãå Í≤ΩÏö∞)\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            if 'Features' in name:\n",
    "                backbone_params.append(param)\n",
    "            else:\n",
    "                new_layer_params.append(param)\n",
    "    \n",
    "    return [\n",
    "        {'params': backbone_params, 'lr': backbone_lr},\n",
    "        {'params': new_layer_params, 'lr': new_layers_lr}\n",
    "    ]\n",
    "\n",
    "# Îã§Î•∏ ÌïôÏäµÎ•† ÏÇ¨Ïö©ÌïòÍ≥† Ïã∂Îã§Î©¥:\n",
    "# param_groups = get_parameter_groups_with_different_lr(net, backbone_lr=1e-5, new_layers_lr=1e-3)\n",
    "# optimizer = SAM(param_groups, torch.optim.SGD, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# best_logloss = float('inf')\n",
    "\n",
    "# # # ÏÜêÏã§ Ìï®Ïàò\n",
    "# # criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# # # ÏòµÌã∞ÎßàÏù¥Ï†Ä\n",
    "# # optimizer = optim.Adam(model.parameters(), lr=CFG['LEARNING_RATE'])\n",
    "\n",
    "\n",
    "# # PMAL\n",
    "# model_params = {'num_classes' : 196}\n",
    "# model = TResnetL368(model_params)\n",
    "# weights_path = \"/kaggle/input/tresnet-stanford-cars-pretrained/stanford_cars_tresnet-l-v2_96_27.pth\"\n",
    "# pretrained_weights = torch.load(weights_path)\n",
    "# model.load_state_dict(pretrained_weights['model'])\n",
    "\n",
    "# net_layers = list(model.children())\n",
    "# classifier = net_layers[1:3]\n",
    "# net_layers = net_layers[0]\n",
    "# net_layers = list(net_layers.children())\n",
    "\n",
    "# net = Network_Wrapper(net_layers, len(class_names), classifier)\n",
    "# # netp = torch.nn.DataParallel(net, device_ids=[0])\n",
    "\n",
    "# net.to(device)\n",
    "# decoder1 = Anti_Noise_Decoder(1, 512).to(device)\n",
    "# decoder2 = Anti_Noise_Decoder(2, 1024).to(device)\n",
    "# decoder3 = Anti_Noise_Decoder(4, 2048).to(device)\n",
    "\n",
    "# #loss\n",
    "# CB_loss = CharbonnierLoss()\n",
    "\n",
    "# #optimizer\n",
    "# base_optimizer = torch.optim.SGD\n",
    "\n",
    "# optimizer = SAM([\n",
    "#         {'params': net.classifier_initial.parameters(), 'lr': 0.002},\n",
    "#         {'params': net.conv_block1.parameters(), 'lr': 0.002},\n",
    "#         {'params': net.classifier1.parameters(), 'lr': 0.002},\n",
    "#         {'params': net.conv_block2.parameters(), 'lr': 0.002},\n",
    "#         {'params': net.classifier2.parameters(), 'lr': 0.002},\n",
    "#         {'params': net.conv_block3.parameters(), 'lr': 0.002},\n",
    "#         {'params': net.classifier3.parameters(), 'lr': 0.002},\n",
    "\n",
    "#         {'params': decoder1.skip.parameters(), 'lr': 0.002},\n",
    "#         {'params': decoder1.process.parameters(), 'lr': 0.002},\n",
    "#         {'params': decoder2.skip.parameters(), 'lr': 0.002},\n",
    "#         {'params': decoder2.process.parameters(), 'lr': 0.002},\n",
    "#         {'params': decoder3.skip.parameters(), 'lr': 0.002},\n",
    "#         {'params': decoder3.process.parameters(), 'lr': 0.002},\n",
    "\n",
    "#         {'params': net.Features.parameters(), 'lr': 0.0002}\n",
    "\n",
    "#     ],\n",
    "#         base_optimizer, adaptive=False, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "# max_val_acc = 0\n",
    "# lr = [0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.0002]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NGlK2nPYCKcr"
   },
   "source": [
    "# Train/ Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 522
    },
    "executionInfo": {
     "elapsed": 263293,
     "status": "error",
     "timestamp": 1747893498648,
     "user": {
      "displayName": "Î∞ïÏßÑÏòÅ",
      "userId": "15299328254354703813"
     },
     "user_tz": -540
    },
    "id": "DwNWkTC3CKcr",
    "outputId": "c3e4f7fa-70b2-4225-bcc9-b185a00f9f99",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "gradient_accumulation = CFG['gradient_accumulation']\n",
    "\n",
    "if not gradient_accumulation:\n",
    "    \n",
    "    # ÌïôÏäµ Î∞è Í≤ÄÏ¶ù Î£®ÌîÑ\n",
    "    for epoch in range(CFG['EPOCHS']):\n",
    "        # Train\n",
    "        net.train()\n",
    "        train_loss = 0.0\n",
    "        train_loss1 = 0\n",
    "        train_loss2 = 0\n",
    "        train_loss3 = 0\n",
    "        train_loss4 = 0\n",
    "        train_loss5 = 0\n",
    "    \n",
    "        for images, targets in tqdm(train_loader, desc=f\"[Epoch {epoch+1}/{CFG['EPOCHS']}] Training\"):\n",
    "            images, targets = images.to(device), targets.to(device)\n",
    "    \n",
    "            for nlr in range(len(optimizer.param_groups)):\n",
    "                optimizer.param_groups[nlr]['lr'] = cosine_anneal_schedule(epoch, CFG['EPOCHS'], lr[nlr])\n",
    "            \n",
    "            sometimes_1 = lambda aug: iaa.Sometimes(0.2, aug)\n",
    "            sometimes_2 = lambda aug: iaa.Sometimes(0.5, aug)\n",
    "    \n",
    "            trans_seq_aug = iaa.Sequential(\n",
    "                [\n",
    "    \n",
    "                    sometimes_1(iaa.Affine(\n",
    "                        scale={\"x\": (0.8, 1.2), \"y\": (0.8, 1.2)},\n",
    "                        translate_percent={\"x\": (-0.2, 0.2), \"y\": (-0.2, 0.2)},\n",
    "                        rotate=(-15, 15),\n",
    "                        shear=(-15, 15),\n",
    "                        order=[0, 1],\n",
    "                        cval=(0, 1),\n",
    "                        mode=ia.ALL\n",
    "                    )),\n",
    "                    sometimes_2(iaa.GaussianBlur((0, 3.0)))\n",
    "                ],\n",
    "                random_order=True\n",
    "            )\n",
    "    \n",
    "            trans_seq = iaa.Sequential(\n",
    "                [\n",
    "    \n",
    "                    iaa.AdditiveGaussianNoise(\n",
    "                        loc=0, scale=(0.0, 0.05), per_channel=0.5\n",
    "                    )\n",
    "                ],\n",
    "                random_order=True\n",
    "            )\n",
    "            # H1 first forward-backward step\n",
    "            enable_running_stats(net)\n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "            inputs1_gt = img_add_noise(images, trans_seq_aug).to(device)\n",
    "            inputs1 = img_add_noise(inputs1_gt, trans_seq).to(device)\n",
    "            output_1, _, _, _, map1, _, _ = net(inputs1)\n",
    "    \n",
    "            loss1_c = CELoss(output_1, targets).mean() * 1\n",
    "            inputs1_syn = decoder1(inputs1, map1)\n",
    "            loss1_g = CB_loss(inputs1_syn, inputs1_gt) * 1\n",
    "    \n",
    "            output_1_syn, _, _, _, _, _, _ = net(inputs1_syn)\n",
    "            loss1_c_syn = CELoss(output_1_syn, targets).mean() * 1\n",
    "    \n",
    "            loss1 = loss1_c + (loss1_g) + loss1_c_syn\n",
    "            loss1.backward()\n",
    "            optimizer.first_step(zero_grad=True)\n",
    "    \n",
    "            # H1 second forward-backward step\n",
    "            disable_running_stats(net)\n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "            output_1, _, _, _, map1, _, _ = net(inputs1)\n",
    "            loss1_c = CELoss(output_1, targets).mean() * 1\n",
    "    \n",
    "            inputs1_syn = decoder1(inputs1, map1)\n",
    "            loss1_g = CB_loss(inputs1_syn, inputs1_gt) * 1\n",
    "    \n",
    "            output_1_syn, _, _, _, _, _, _ = net(inputs1_syn)\n",
    "            loss1_c_syn = CELoss(output_1_syn, targets).mean() * 1\n",
    "    \n",
    "            loss1_ = loss1_c + loss1_g + loss1_c_syn\n",
    "            loss1_.backward()\n",
    "            optimizer.second_step(zero_grad=True)\n",
    "    \n",
    "            loss1 = loss1.cpu()\n",
    "            loss1_g = loss1_g.cpu()\n",
    "    \n",
    "            del output_1\n",
    "            del output_1_syn\n",
    "            del loss1_\n",
    "            del loss1_c\n",
    "            del loss1_c_syn\n",
    "            del inputs1\n",
    "            del inputs1_gt\n",
    "            del inputs1_syn\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "            # H2\n",
    "            # H2 first forward-backward step\n",
    "            enable_running_stats(net)\n",
    "            optimizer.zero_grad()\n",
    "            inputs2_gt = img_add_noise(images, trans_seq_aug).to(device)\n",
    "            inputs2 = img_add_noise(inputs2_gt, trans_seq).to(device)\n",
    "            _, output_2, _, _, _, map2, _ = net(inputs2)\n",
    "            loss2_c = CELoss(output_2, targets).mean() * 1\n",
    "    \n",
    "            inputs2_syn = decoder2(inputs2, map2)\n",
    "            loss2_g = CB_loss(inputs2_syn, inputs2_gt) * 1\n",
    "    \n",
    "            _, output_2_syn, _, _, _, _, _ = net(inputs2_syn)\n",
    "            loss2_c_syn = CELoss(output_2_syn, targets).mean() * 1\n",
    "    \n",
    "            loss2 = loss2_c + loss2_g + loss2_c_syn\n",
    "            loss2.backward()\n",
    "            optimizer.first_step(zero_grad=True)\n",
    "    \n",
    "            # H2 second forward-backward step\n",
    "            disable_running_stats(net)\n",
    "            optimizer.zero_grad()\n",
    "            _, output_2, _, _, _, map2, _ = net(inputs2)\n",
    "            loss2_c = CELoss(output_2, targets).mean() * 1\n",
    "    \n",
    "            inputs2_syn = decoder2(inputs2, map2)\n",
    "            loss2_g = CB_loss(inputs2_syn, inputs2_gt) * 1\n",
    "    \n",
    "            _, output_2_syn, _, _, _, _, _ = net(inputs2_syn)\n",
    "            loss2_c_syn = CELoss(output_2_syn, targets).mean() * 1\n",
    "    \n",
    "            loss2_ = loss2_c + (loss2_g) + loss2_c_syn\n",
    "            loss2_.backward()\n",
    "            optimizer.second_step(zero_grad=True)\n",
    "    \n",
    "            loss2 = loss2.cpu()\n",
    "            loss2_g = loss2_g.cpu()\n",
    "            del output_2\n",
    "            del output_2_syn\n",
    "            del loss2_\n",
    "            del loss2_c\n",
    "            del loss2_c_syn\n",
    "            del inputs2\n",
    "            del inputs2_gt\n",
    "            del inputs2_syn\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "            # H3\n",
    "            # H3 first forward-backward step\n",
    "            enable_running_stats(net)\n",
    "            optimizer.zero_grad()\n",
    "            inputs3_gt = img_add_noise(images, trans_seq_aug).to(device)\n",
    "            inputs3 = img_add_noise(inputs3_gt, trans_seq).to(device)\n",
    "            _, _, output_3, _, _, _, map3 = net(inputs3)\n",
    "            loss3_c = CELoss(output_3, targets).mean() * 1\n",
    "    \n",
    "            inputs3_syn = decoder3(inputs3, map3)\n",
    "            loss3_g = CB_loss(inputs3_syn, inputs3_gt) * 1\n",
    "    \n",
    "            _, _, output_3_syn, _, _, _, _ = net(inputs3_syn)\n",
    "            loss3_c_syn = CELoss(output_3_syn, targets).mean() * 1\n",
    "    \n",
    "            loss3 = loss3_c + (loss3_g) + loss3_c_syn\n",
    "            loss3.backward()\n",
    "            optimizer.first_step(zero_grad=True)\n",
    "    \n",
    "            # H3 second forward-backward step\n",
    "            disable_running_stats(net)\n",
    "            optimizer.zero_grad()\n",
    "            _, _, output_3, _, _, _, map3 = net(inputs3)\n",
    "            loss3_c = CELoss(output_3, targets).mean() * 1\n",
    "    \n",
    "            inputs3_syn = decoder3(inputs3, map3)\n",
    "            loss3_g = CB_loss(inputs3_syn, inputs3_gt) * 1\n",
    "    \n",
    "            _, _, output_3_syn, _, _, _, _ = net(inputs3_syn)\n",
    "            loss3_c_syn = CELoss(output_3_syn, targets).mean() * 1\n",
    "    \n",
    "            loss3_ = loss3_c + (loss3_g) + loss3_c_syn\n",
    "            loss3_.backward()\n",
    "            optimizer.second_step(zero_grad=True)\n",
    "    \n",
    "            loss3 = loss3.cpu()\n",
    "            loss3_g = loss3_g.cpu()\n",
    "            del output_3\n",
    "            del output_3_syn\n",
    "            del loss3_\n",
    "            del loss3_c\n",
    "            del loss3_c_syn\n",
    "            del inputs3\n",
    "            del inputs3_gt\n",
    "            del inputs3_syn\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "            # H4\n",
    "            # H4 first forward-backward step\n",
    "            enable_running_stats(net)\n",
    "            optimizer.zero_grad()\n",
    "            output_1_final, output_2_final, output_3_final, output_ORI, _, _, _ = net(images)\n",
    "            ORI_loss = CELoss(output_1_final, targets).mean() + \\\n",
    "                        CELoss(output_2_final, targets).mean() + \\\n",
    "                        CELoss(output_3_final, targets).mean() + \\\n",
    "                        CELoss(output_ORI, targets).mean() * 2\n",
    "            # ÏÜêÏã§ Í≥ÑÏÇ∞ Ï†ÑÏóê targets Í≤ÄÏÇ¨\n",
    "    \n",
    "            ORI_loss.backward()\n",
    "            optimizer.first_step(zero_grad=True)\n",
    "    \n",
    "            # H4 second forward-backward step\n",
    "            disable_running_stats(net)\n",
    "            optimizer.zero_grad()\n",
    "            output_1_final, output_2_final, output_3_final, output_ORI, _, _, _ = net(images)\n",
    "            ORI_loss_ = CELoss(output_1_final, targets).mean() + \\\n",
    "                        CELoss(output_2_final, targets).mean() + \\\n",
    "                        CELoss(output_3_final, targets).mean() + \\\n",
    "                        CELoss(output_ORI, targets).mean() * 2\n",
    "            ORI_loss_.backward()\n",
    "            optimizer.second_step(zero_grad=True)\n",
    "    \n",
    "            ORI_loss = ORI_loss.cpu()\n",
    "            del output_1_final\n",
    "            del output_2_final\n",
    "            del output_3_final\n",
    "            output_ORI = output_ORI.cpu()\n",
    "            targets = targets.cpu()\n",
    "            del images\n",
    "            del ORI_loss_\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "            train_loss += (loss1.item() + loss2.item() + loss3.item() + ORI_loss.item()) # \n",
    "            train_loss1 += loss1.item()\n",
    "            train_loss2 += loss2.item()\n",
    "            train_loss3 += loss3.item()\n",
    "            train_loss4 += (loss1_g.item() + loss2_g.item() + loss3_g.item())\n",
    "            train_loss5 += ORI_loss.item()\n",
    "        total = len(train_loader)\n",
    "        avg_train_loss = train_loss / total\n",
    "        avg_train_loss1 = train_loss1 / total\n",
    "        avg_train_loss2 = train_loss2 / total\n",
    "        avg_train_loss3 = train_loss3 / total\n",
    "        avg_train_loss4 = train_loss4 / total\n",
    "        avg_train_loss5 = train_loss5 / total\n",
    "    \n",
    "        # Validation\n",
    "        net.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        correct_com = 0\n",
    "        total = 0\n",
    "        all_probs = []\n",
    "        all_probs_com = []\n",
    "        all_labels = []\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            for images, labels in tqdm(val_loader, desc=f\"[Epoch {epoch+1}/{CFG['EPOCHS']}] Validation\"):\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "    \n",
    "                output_1, output_2, output_3, output_ORI, _, _, _ = net(images)\n",
    "    \n",
    "                outputs_com = output_1.cpu() + output_2.cpu() + output_3.cpu() + output_ORI.cpu()\n",
    "                loss = CELoss(output_ORI, labels)\n",
    "                val_loss += loss.item()\n",
    "    \n",
    "                # Accuracy\n",
    "                _, preds = torch.max(output_ORI, 1)\n",
    "                _, preds_com = torch.max(outputs_com, 1)\n",
    "    \n",
    "                correct += (preds == labels).sum().item()\n",
    "                correct_com += (preds_com == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "    \n",
    "                # LogLoss\n",
    "                probs = F.softmax(output_ORI, dim=1)\n",
    "                all_probs.extend(probs.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "                probs_com = F.softmax(outputs_com, dim=1)\n",
    "                all_probs_com.extend(probs_com.cpu().numpy())\n",
    "    \n",
    "    \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_accuracy = 100 * correct / total\n",
    "        val_com_accuracy = 100 * correct_com / total\n",
    "        val_logloss = log_loss(all_labels, all_probs, labels=list(range(len(class_names))))\n",
    "        val_com_logloss = log_loss(all_labels, all_probs_com, labels=list(range(len(class_names))))\n",
    "        del images\n",
    "        del loss\n",
    "        del targets\n",
    "        del output_1\n",
    "        del output_2\n",
    "        del output_3\n",
    "        del output_ORI\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # wandb \n",
    "        wandb.log({\n",
    "            \"train_loss\": avg_train_loss,\n",
    "            \"train_loss1\": avg_train_loss1,\n",
    "            \"train_loss2\": avg_train_loss2,\n",
    "            \"train_loss3\": avg_train_loss3,\n",
    "            \"train_loss4\": avg_train_loss4,\n",
    "            \"train_loss5\": avg_train_loss5,\n",
    "            \"val_loss\": avg_val_loss,\n",
    "            \"val_accuracy\": val_accuracy,\n",
    "            \"val_com_accuracy\": val_com_accuracy,\n",
    "            \"val_logloss\": val_logloss,\n",
    "            \"val_com_logloss\": val_com_logloss,\n",
    "        })\n",
    "        \n",
    "        # Í≤∞Í≥º Ï∂úÎ†•\n",
    "        print(f\"Train Loss : {avg_train_loss:.4f} || Valid Loss : {avg_val_loss:.4f} | Valid Accuracy : {val_accuracy:.4f}%\")\n",
    "    \n",
    "        # Best model Ï†ÄÏû•\n",
    "        if val_logloss < best_logloss:\n",
    "            best_logloss = val_logloss\n",
    "            torch.save(net.state_dict(), f'best_model.pth')\n",
    "            print(f\"üì¶ Best model saved at epoch {epoch+1} (logloss: {val_logloss:.4f})\")\n",
    "            torch.save(decoder1, f'decoder1.pth')\n",
    "            torch.save(decoder1, f'decoder2.pth')\n",
    "            torch.save(decoder1, f'decoder3.pth')\n",
    "    \n",
    "    \n",
    "        early_stopping(val_logloss, model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(f\"üõë Early stopping triggered at epoch {epoch+1}\")\n",
    "            break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Gradient Accumulation ÏÑ§Ï†ï\n",
    "if gradient_accumulation:\n",
    "\n",
    "    accumulation_steps = CFG['accumulation_steps']  # ÏõêÌïòÎäî accumulation step ÏàòÎ°ú Ï°∞Ï†ï\n",
    "    \n",
    "    # ÌïôÏäµ Î∞è Í≤ÄÏ¶ù Î£®ÌîÑ\n",
    "    for epoch in range(CFG['EPOCHS']):\n",
    "        # Train\n",
    "        net.train()\n",
    "        train_loss = 0.0\n",
    "        train_loss1 = 0\n",
    "        train_loss2 = 0\n",
    "        train_loss3 = 0\n",
    "        train_loss4 = 0\n",
    "        train_loss5 = 0\n",
    "        \n",
    "        # Gradient accumulationÏùÑ ÏúÑÌïú Î≥ÄÏàòÎì§\n",
    "        accumulated_loss1 = 0\n",
    "        accumulated_loss2 = 0\n",
    "        accumulated_loss3 = 0\n",
    "        accumulated_loss4 = 0\n",
    "    \n",
    "        for batch_idx, (images, targets) in enumerate(tqdm(train_loader, desc=f\"[Epoch {epoch+1}/{CFG['EPOCHS']}] Training\")):\n",
    "            images, targets = images.to(device), targets.to(device)\n",
    "    \n",
    "            for nlr in range(len(optimizer.param_groups)):\n",
    "                optimizer.param_groups[nlr]['lr'] = cosine_anneal_schedule(epoch, CFG['EPOCHS'], lr[nlr])\n",
    "            \n",
    "            sometimes_1 = lambda aug: iaa.Sometimes(0.2, aug)\n",
    "            sometimes_2 = lambda aug: iaa.Sometimes(0.5, aug)\n",
    "    \n",
    "            trans_seq_aug = iaa.Sequential(\n",
    "                [\n",
    "                    sometimes_1(iaa.Affine(\n",
    "                        scale={\"x\": (0.8, 1.2), \"y\": (0.8, 1.2)},\n",
    "                        translate_percent={\"x\": (-0.2, 0.2), \"y\": (-0.2, 0.2)},\n",
    "                        rotate=(-15, 15),\n",
    "                        shear=(-15, 15),\n",
    "                        order=[0, 1],\n",
    "                        cval=(0, 1),\n",
    "                        mode=ia.ALL\n",
    "                    )),\n",
    "                    sometimes_2(iaa.GaussianBlur((0, 3.0)))\n",
    "                ],\n",
    "                random_order=True\n",
    "            )\n",
    "    \n",
    "            trans_seq = iaa.Sequential(\n",
    "                [\n",
    "                    iaa.AdditiveGaussianNoise(\n",
    "                        loc=0, scale=(0.0, 0.05), per_channel=0.5\n",
    "                    )\n",
    "                ],\n",
    "                random_order=True\n",
    "            )\n",
    "            \n",
    "            # H1 first forward-backward step\n",
    "            enable_running_stats(net)\n",
    "            if batch_idx % accumulation_steps == 0:\n",
    "                optimizer.zero_grad()\n",
    "    \n",
    "            inputs1_gt = img_add_noise(images, trans_seq_aug).to(device)\n",
    "            inputs1 = img_add_noise(inputs1_gt, trans_seq).to(device)\n",
    "            output_1, _, _, _, map1, _, _ = net(inputs1)\n",
    "    \n",
    "            loss1_c = CELoss(output_1, targets).mean() * 1\n",
    "            inputs1_syn = decoder1(inputs1, map1)\n",
    "            loss1_g = CB_loss(inputs1_syn, inputs1_gt) * 1\n",
    "    \n",
    "            output_1_syn, _, _, _, _, _, _ = net(inputs1_syn)\n",
    "            loss1_c_syn = CELoss(output_1_syn, targets).mean() * 1\n",
    "    \n",
    "            loss1 = (loss1_c + loss1_g + loss1_c_syn) / accumulation_steps  # accumulationÏúºÎ°ú ÎÇòÎàÑÍ∏∞\n",
    "            loss1.backward()\n",
    "            \n",
    "            accumulated_loss1 += loss1.item() * accumulation_steps  # Ïã§Ï†ú loss Í∞í Ï†ÄÏû•\n",
    "            \n",
    "            if (batch_idx + 1) % accumulation_steps == 0 or (batch_idx + 1) == len(train_loader):\n",
    "                optimizer.first_step(zero_grad=True)\n",
    "    \n",
    "            # H1 second forward-backward step\n",
    "            disable_running_stats(net)\n",
    "            if batch_idx % accumulation_steps == 0:\n",
    "                optimizer.zero_grad()\n",
    "    \n",
    "            output_1, _, _, _, map1, _, _ = net(inputs1)\n",
    "            loss1_c = CELoss(output_1, targets).mean() * 1\n",
    "    \n",
    "            inputs1_syn = decoder1(inputs1, map1)\n",
    "            loss1_g = CB_loss(inputs1_syn, inputs1_gt) * 1\n",
    "    \n",
    "            output_1_syn, _, _, _, _, _, _ = net(inputs1_syn)\n",
    "            loss1_c_syn = CELoss(output_1_syn, targets).mean() * 1\n",
    "    \n",
    "            loss1_ = (loss1_c + loss1_g + loss1_c_syn) / accumulation_steps\n",
    "            loss1_.backward()\n",
    "            \n",
    "            if (batch_idx + 1) % accumulation_steps == 0 or (batch_idx + 1) == len(train_loader):\n",
    "                optimizer.second_step(zero_grad=True)\n",
    "    \n",
    "            loss1_g_cpu = loss1_g.cpu()\n",
    "    \n",
    "            del output_1, output_1_syn, loss1_, loss1_c, loss1_c_syn\n",
    "            del inputs1, inputs1_gt, inputs1_syn\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "            # H2 first forward-backward step\n",
    "            enable_running_stats(net)\n",
    "            if batch_idx % accumulation_steps == 0:\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "            inputs2_gt = img_add_noise(images, trans_seq_aug).to(device)\n",
    "            inputs2 = img_add_noise(inputs2_gt, trans_seq).to(device)\n",
    "            _, output_2, _, _, _, map2, _ = net(inputs2)\n",
    "            loss2_c = CELoss(output_2, targets).mean() * 1\n",
    "    \n",
    "            inputs2_syn = decoder2(inputs2, map2)\n",
    "            loss2_g = CB_loss(inputs2_syn, inputs2_gt) * 1\n",
    "    \n",
    "            _, output_2_syn, _, _, _, _, _ = net(inputs2_syn)\n",
    "            loss2_c_syn = CELoss(output_2_syn, targets).mean() * 1\n",
    "    \n",
    "            loss2 = (loss2_c + loss2_g + loss2_c_syn) / accumulation_steps\n",
    "            loss2.backward()\n",
    "            \n",
    "            accumulated_loss2 += loss2.item() * accumulation_steps\n",
    "            \n",
    "            if (batch_idx + 1) % accumulation_steps == 0 or (batch_idx + 1) == len(train_loader):\n",
    "                optimizer.first_step(zero_grad=True)\n",
    "    \n",
    "            # H2 second forward-backward step\n",
    "            disable_running_stats(net)\n",
    "            if batch_idx % accumulation_steps == 0:\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "            _, output_2, _, _, _, map2, _ = net(inputs2)\n",
    "            loss2_c = CELoss(output_2, targets).mean() * 1\n",
    "    \n",
    "            inputs2_syn = decoder2(inputs2, map2)\n",
    "            loss2_g = CB_loss(inputs2_syn, inputs2_gt) * 1\n",
    "    \n",
    "            _, output_2_syn, _, _, _, _, _ = net(inputs2_syn)\n",
    "            loss2_c_syn = CELoss(output_2_syn, targets).mean() * 1\n",
    "    \n",
    "            loss2_ = (loss2_c + loss2_g + loss2_c_syn) / accumulation_steps\n",
    "            loss2_.backward()\n",
    "            \n",
    "            if (batch_idx + 1) % accumulation_steps == 0 or (batch_idx + 1) == len(train_loader):\n",
    "                optimizer.second_step(zero_grad=True)\n",
    "    \n",
    "            loss2_g_cpu = loss2_g.cpu()\n",
    "            \n",
    "            del output_2, output_2_syn, loss2_, loss2_c, loss2_c_syn\n",
    "            del inputs2, inputs2_gt, inputs2_syn\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "            # H3 first forward-backward step\n",
    "            enable_running_stats(net)\n",
    "            if batch_idx % accumulation_steps == 0:\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "            inputs3_gt = img_add_noise(images, trans_seq_aug).to(device)\n",
    "            inputs3 = img_add_noise(inputs3_gt, trans_seq).to(device)\n",
    "            _, _, output_3, _, _, _, map3 = net(inputs3)\n",
    "            loss3_c = CELoss(output_3, targets).mean() * 1\n",
    "    \n",
    "            inputs3_syn = decoder3(inputs3, map3)\n",
    "            loss3_g = CB_loss(inputs3_syn, inputs3_gt) * 1\n",
    "    \n",
    "            _, _, output_3_syn, _, _, _, _ = net(inputs3_syn)\n",
    "            loss3_c_syn = CELoss(output_3_syn, targets).mean() * 1\n",
    "    \n",
    "            loss3 = (loss3_c + loss3_g + loss3_c_syn) / accumulation_steps\n",
    "            loss3.backward()\n",
    "            \n",
    "            accumulated_loss3 += loss3.item() * accumulation_steps\n",
    "            \n",
    "            if (batch_idx + 1) % accumulation_steps == 0 or (batch_idx + 1) == len(train_loader):\n",
    "                optimizer.first_step(zero_grad=True)\n",
    "    \n",
    "            # H3 second forward-backward step\n",
    "            disable_running_stats(net)\n",
    "            if batch_idx % accumulation_steps == 0:\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "            _, _, output_3, _, _, _, map3 = net(inputs3)\n",
    "            loss3_c = CELoss(output_3, targets).mean() * 1\n",
    "    \n",
    "            inputs3_syn = decoder3(inputs3, map3)\n",
    "            loss3_g = CB_loss(inputs3_syn, inputs3_gt) * 1\n",
    "    \n",
    "            _, _, output_3_syn, _, _, _, _ = net(inputs3_syn)\n",
    "            loss3_c_syn = CELoss(output_3_syn, targets).mean() * 1\n",
    "    \n",
    "            loss3_ = (loss3_c + loss3_g + loss3_c_syn) / accumulation_steps\n",
    "            loss3_.backward()\n",
    "            \n",
    "            if (batch_idx + 1) % accumulation_steps == 0 or (batch_idx + 1) == len(train_loader):\n",
    "                optimizer.second_step(zero_grad=True)\n",
    "    \n",
    "            loss3_g_cpu = loss3_g.cpu()\n",
    "            \n",
    "            del output_3, output_3_syn, loss3_, loss3_c, loss3_c_syn\n",
    "            del inputs3, inputs3_gt, inputs3_syn\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "            # H4 first forward-backward step\n",
    "            enable_running_stats(net)\n",
    "            if batch_idx % accumulation_steps == 0:\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "            output_1_final, output_2_final, output_3_final, output_ORI, _, _, _ = net(images)\n",
    "            ORI_loss = (CELoss(output_1_final, targets).mean() + \\\n",
    "                       CELoss(output_2_final, targets).mean() + \\\n",
    "                       CELoss(output_3_final, targets).mean() + \\\n",
    "                       CELoss(output_ORI, targets).mean() * 2) / accumulation_steps\n",
    "    \n",
    "            ORI_loss.backward()\n",
    "            \n",
    "            accumulated_loss4 += ORI_loss.item() * accumulation_steps\n",
    "            \n",
    "            if (batch_idx + 1) % accumulation_steps == 0 or (batch_idx + 1) == len(train_loader):\n",
    "                optimizer.first_step(zero_grad=True)\n",
    "    \n",
    "            # H4 second forward-backward step\n",
    "            disable_running_stats(net)\n",
    "            if batch_idx % accumulation_steps == 0:\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "            output_1_final, output_2_final, output_3_final, output_ORI, _, _, _ = net(images)\n",
    "            ORI_loss_ = (CELoss(output_1_final, targets).mean() + \\\n",
    "                        CELoss(output_2_final, targets).mean() + \\\n",
    "                        CELoss(output_3_final, targets).mean() + \\\n",
    "                        CELoss(output_ORI, targets).mean() * 2) / accumulation_steps\n",
    "            ORI_loss_.backward()\n",
    "            \n",
    "            if (batch_idx + 1) % accumulation_steps == 0 or (batch_idx + 1) == len(train_loader):\n",
    "                optimizer.second_step(zero_grad=True)\n",
    "    \n",
    "            del output_1_final, output_2_final, output_3_final, output_ORI\n",
    "            del images, targets, ORI_loss_\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "            # accumulation stepÏù¥ ÏôÑÎ£åÎêòÏóàÏùÑ ÎïåÎßå loss ÎàÑÏ†Å\n",
    "            if (batch_idx + 1) % accumulation_steps == 0 or (batch_idx + 1) == len(train_loader):\n",
    "                train_loss += (accumulated_loss1 + accumulated_loss2 + accumulated_loss3 + accumulated_loss4)\n",
    "                train_loss1 += accumulated_loss1\n",
    "                train_loss2 += accumulated_loss2\n",
    "                train_loss3 += accumulated_loss3\n",
    "                train_loss4 += (loss1_g_cpu.item() + loss2_g_cpu.item() + loss3_g_cpu.item())\n",
    "                train_loss5 += accumulated_loss4\n",
    "                \n",
    "                # ÎàÑÏ†Å Î≥ÄÏàò Ï¥àÍ∏∞Ìôî\n",
    "                accumulated_loss1 = 0\n",
    "                accumulated_loss2 = 0\n",
    "                accumulated_loss3 = 0\n",
    "                accumulated_loss4 = 0\n",
    "    \n",
    "        # ÌèâÍ∑† Í≥ÑÏÇ∞ Ïãú Ïã§Ï†ú step ÏàòÎ°ú ÎÇòÎàÑÍ∏∞\n",
    "        actual_steps = (len(train_loader) + accumulation_steps - 1) // accumulation_steps\n",
    "        avg_train_loss = train_loss / actual_steps\n",
    "        avg_train_loss1 = train_loss1 / actual_steps\n",
    "        avg_train_loss2 = train_loss2 / actual_steps\n",
    "        avg_train_loss3 = train_loss3 / actual_steps\n",
    "        avg_train_loss4 = train_loss4 / actual_steps\n",
    "        avg_train_loss5 = train_loss5 / actual_steps\n",
    "    \n",
    "        # Validation (Í∏∞Ï°¥Í≥º ÎèôÏùº)\n",
    "        net.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        correct_com = 0\n",
    "        total = 0\n",
    "        all_probs = []\n",
    "        all_probs_com = []\n",
    "        all_labels = []\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            for images, labels in tqdm(val_loader, desc=f\"[Epoch {epoch+1}/{CFG['EPOCHS']}] Validation\"):\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "    \n",
    "                output_1, output_2, output_3, output_ORI, _, _, _ = net(images)\n",
    "    \n",
    "                outputs_com = output_1.cpu() + output_2.cpu() + output_3.cpu() + output_ORI.cpu()\n",
    "                loss = CELoss(output_ORI, labels)\n",
    "                val_loss += loss.item()\n",
    "    \n",
    "                # Accuracy\n",
    "                _, preds = torch.max(output_ORI, 1)\n",
    "                _, preds_com = torch.max(outputs_com, 1)\n",
    "    \n",
    "                correct += (preds == labels).sum().item()\n",
    "                correct_com += (preds_com == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "    \n",
    "                # LogLoss\n",
    "                probs = F.softmax(output_ORI, dim=1)\n",
    "                all_probs.extend(probs.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "                probs_com = F.softmax(outputs_com, dim=1)\n",
    "                all_probs_com.extend(probs_com.cpu().numpy())\n",
    "    \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_accuracy = 100 * correct / total\n",
    "        val_com_accuracy = 100 * correct_com / total\n",
    "        val_logloss = log_loss(all_labels, all_probs, labels=list(range(len(class_names))))\n",
    "        val_com_logloss = log_loss(all_labels, all_probs_com, labels=list(range(len(class_names))))\n",
    "        \n",
    "        del images, loss, output_1, output_2, output_3, output_ORI\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # wandb \n",
    "        wandb.log({\n",
    "            \"train_loss\": avg_train_loss,\n",
    "            \"train_loss1\": avg_train_loss1,\n",
    "            \"train_loss2\": avg_train_loss2,\n",
    "            \"train_loss3\": avg_train_loss3,\n",
    "            \"train_loss4\": avg_train_loss4,\n",
    "            \"train_loss5\": avg_train_loss5,\n",
    "            \"val_loss\": avg_val_loss,\n",
    "            \"val_accuracy\": val_accuracy,\n",
    "            \"val_com_accuracy\": val_com_accuracy,\n",
    "            \"val_logloss\": val_logloss,\n",
    "            \"val_com_logloss\": val_com_logloss,\n",
    "        })\n",
    "        \n",
    "        # Í≤∞Í≥º Ï∂úÎ†•\n",
    "        print(f\"Train Loss : {avg_train_loss:.4f} || Valid Loss : {avg_val_loss:.4f} | Valid Accuracy : {val_accuracy:.4f}%\")\n",
    "    \n",
    "        # Best model Ï†ÄÏû•\n",
    "        if val_logloss < best_logloss:\n",
    "            best_logloss = val_logloss\n",
    "            torch.save(net.state_dict(), f'best_model.pth')\n",
    "            print(f\"üì¶ Best model saved at epoch {epoch+1} (logloss: {val_logloss:.4f})\")\n",
    "            torch.save(decoder1, f'decoder1.pth')\n",
    "            torch.save(decoder2, f'decoder2.pth')  # decoder2Î°ú ÏàòÏ†ï\n",
    "            torch.save(decoder3, f'decoder3.pth')  # decoder3ÏúºÎ°ú ÏàòÏ†ï\n",
    "    \n",
    "        early_stopping(val_logloss, model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(f\"üõë Early stopping triggered at epoch {epoch+1}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TwGAOAiKCKcr"
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 379040,
     "status": "aborted",
     "timestamp": 1747893498644,
     "user": {
      "displayName": "Î∞ïÏßÑÏòÅ",
      "userId": "15299328254354703813"
     },
     "user_tz": -540
    },
    "id": "4vS6URwRCKcr",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_dataset = CustomImageDataset(test_root, transform=val_transform, is_test=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 379037,
     "status": "aborted",
     "timestamp": 1747893498646,
     "user": {
      "displayName": "Î∞ïÏßÑÏòÅ",
      "userId": "15299328254354703813"
     },
     "user_tz": -540
    },
    "id": "i8XWppr-CKcr",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Ï†ÄÏû•Îêú Î™®Îç∏ Î°úÎìú\n",
    "model = TResnetL368(model_params)\n",
    "model.load_state_dict(torch.load('best_model.pth', map_location=device))\n",
    "net_layers = list(model.children())\n",
    "classifier = net_layers[1:3]\n",
    "net_layers = net_layers[0]\n",
    "net_layers = list(net_layers.children())\n",
    "net = Network_Wrapper(net_layers, len(class_names), classifier)\n",
    "net.to(device)\n",
    "# Ï∂îÎ°†\n",
    "net.eval()\n",
    "results = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images in test_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = net(images)\n",
    "        probs = F.softmax(outputs, dim=1)\n",
    "\n",
    "        # Í∞Å Î∞∞ÏπòÏùò ÌôïÎ•†ÏùÑ Î¶¨Ïä§Ìä∏Î°ú Î≥ÄÌôò\n",
    "        for prob in probs.cpu():  # prob: (num_classes,)\n",
    "            result = {\n",
    "                class_names[i]: prob[i].item()\n",
    "                for i in range(len(class_names))\n",
    "            }\n",
    "            results.append(result)\n",
    "\n",
    "pred = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HKj-nq9RCKcr"
   },
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 379034,
     "status": "aborted",
     "timestamp": 1747893498647,
     "user": {
      "displayName": "Î∞ïÏßÑÏòÅ",
      "userId": "15299328254354703813"
     },
     "user_tz": -540
    },
    "id": "9VcLATLfCKcr",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "submission = pd.read_csv('/kaggle/input/car-classification/sample_submission.csv', encoding='utf-8-sig')\n",
    "\n",
    "# 'ID' Ïª¨ÎüºÏùÑ Ï†úÏô∏Ìïú ÌÅ¥ÎûòÏä§ Ïª¨Îüº Ï†ïÎ†¨\n",
    "class_columns = submission.columns[1:]\n",
    "pred = pred[class_columns]\n",
    "\n",
    "submission[class_columns] = pred.values\n",
    "submission.to_csv('baseline_submission.csv', index=False, encoding='utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 1053211,
     "sourceId": 1771962,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7483280,
     "sourceId": 11904337,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7492523,
     "sourceId": 11918208,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
